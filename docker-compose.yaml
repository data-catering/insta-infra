services:
  activemq:
    container_name: activemq
    environment:
      - "ARTEMIS_USER=${ARTEMIS_USER:-artemis}"
      - "ARTEMIS_PASSWORD=${ARTEMIS_PASSWORD:-artemis}"
    healthcheck:
      interval: 15s
      retries: 3
      test: [CMD-SHELL, "curl -k -f http://localhost:8161/admin"]
      timeout: 5s
    image: "apache/activemq-artemis:${ACTIVEMQ_VERSION:-2.39.0}"
    ports:
      - "61616:61616"
      - "8161:8161"
  airflow:
    command: standalone
    container_name: airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW_UID=50000
      - "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres/airflow"
      - AIRFLOW__CORE__FERNET_KEY=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=true
      - "AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 30s
      test: [CMD, curl, --fail, "http://localhost:8080/health"]
      timeout: 10s
    image: "apache/airflow:${AIRFLOW_VERSION:-2.10.5}"
    ports:
      - "8081:8080"
    restart: always
    user: "50000:0"
    volumes:
      - "./data/airflow/dags:/opt/airflow/dags"
  airflow-init:
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources=false
        if (( mem_available < 4000 )) ; then
          echo
          echo -e \033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m
          echo At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))
          echo
          warning_resources=true
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e \033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m
          echo At least 2 CPUs recommended. You have $${cpus_available}
          echo
          warning_resources=true
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e \033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m
          echo At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))
          echo
          warning_resources=true
        fi
        if [[ $${warning_resources} == true ]]; then
          echo
          echo -e \033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m
          echo Please follow the instructions to increase amount of resources available:
          echo    https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R 50000:0 /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: /bin/bash
    # yamllint enable rule:line-length
    environment:
      - AIRFLOW_UID=50000
      - "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres/airflow"
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - "_AIRFLOW_WWW_USER_USERNAME=${AIRFLOW_USER:-airflow}"
      - "_AIRFLOW_WWW_USER_PASSWORD=${AIRFLOW_PASSWORD:-airflow}"
      - _PIP_ADDITIONAL_REQUIREMENTS=
    image: "apache/airflow:${AIRFLOW_VERSION:-2.10.5}"
    user: "0:0"
  amundsen:
    command: gunicorn -w 2 --bind :5000 amundsen_application.wsgi
    container_name: amundsen
    depends_on:
      - amundsen-metadata
      - amundsen-search
    environment:
      - SEARCHSERVICE_BASE=http://amundsen-search:5000
      - METADATASERVICE_BASE=http://amundsen-metadata:5000
      - FRONTEND_SVC_CONFIG_MODULE_CLASS=amundsen_application.config.TestConfig
    image: "amundsendev/amundsen-frontend:${AMUNDSEN_FRONTEND_VERSION:-4.3.0}"
    ports:
      - "5003:5000"
  amundsen-metadata:
    command: gunicorn -w 2 --bind :5000 metadata_service.metadata_wsgi
    container_name: amundsen-metadata
    depends_on:
      - amundsen-neo4j
    environment:
      - PROXY_HOST=bolt://amundsen-neo4j
      - PROXY_ENCRYPTED=True
      - PROXY_VALIDATE_SSL=False
    image: "amundsendev/amundsen-metadata:${AMUNDSEN_METADATA_VERSION:-3.13.0}"
    ports:
      - "5002:5000"
  amundsen-neo4j:
    container_name: amundsen-neo4j
    environment:
      - NEO4J_AUTH=none
    image: "datacatering/neo4j:3.5.35"
    ports:
      - "7474:7474"
      - "7687:7687"
    ulimits:
      nofile:
        hard: 40000
        soft: 40000
    volumes:
      - "./data/amundsen-neo4j/conf:/var/lib/neo4j/conf"
  amundsen-search:
    command: gunicorn -w 2 --bind :5000 search_service.search_wsgi
    container_name: amundsen-search
    depends_on:
      - elasticsearch
    environment:
      - PROXY_ENDPOINT=elasticsearch
    image: "amundsendev/amundsen-search:${AMUNDSEN_SEARCH_VERSION:-4.2.0}"
    ports:
      - "5001:5000"
  blazer:
    command: sh -c "rails db:migrate && puma -C /app/config/puma.rb"
    container_name: blazer
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - DATABASE_URL=postgres://postgres:postgres@postgres:5432/blazer
      - DATABASE_CUSTOMER_URL=postgres://postgres:postgres@postgres:5432/customer
    image: "ankane/blazer:${BLAZER_VERSION:-v3.1.0}"
    ports:
      - "8080:8080"
    volumes:
      - "./data/blazer/blazer.yml:/app/config/blazer.yml"
  cassandra:
    command: [-c, /tmp/scripts/init.sh]
    container_name: cassandra-data
    depends_on:
      cassandra-server:
        condition: service_healthy
    entrypoint: /bin/bash
    environment:
      - DS_LICENSE=accept
    image: "datacatering/dse-server:${CASSANDRA_VERSION:-6.8.48}"
    volumes:
      - "./data/cassandra/init.sh:/tmp/scripts/init.sh"
      - "${CASSANDRA_DATA:-./data/cassandra/data}:/tmp/data"
  cassandra-server:
    cap_add:
      - IPC_LOCK
    container_name: cassandra
    environment:
      - DS_LICENSE=accept
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD-SHELL, "[ $$(nodetool statusgossip) = running ]"]
      timeout: 10s
    image: "datacatering/dse-server:${CASSANDRA_VERSION:-6.8.48}"
    ports:
      - "9042:9042"
    ulimits:
      memlock: -1
  clickhouse:
    command: [/bin/bash, -c, /tmp/scripts/init.sh]
    container_name: clickhouse-data
    depends_on:
      clickhouse-server:
        condition: service_healthy
    hostname: clickhouse
    image: "clickhouse/clickhouse-server:${CLICKHOUSE_VERSION:-25.1.5}"
    user: "101:101"
    volumes:
      - "./data/clickhouse/init.sh:/tmp/scripts/init.sh"
      - "${CLICKHOUSE_DATA:-./data/clickhouse/data}:/tmp/data"
  clickhouse-server:
    container_name: clickhouse
    depends_on:
      postgres:
        condition: service_completed_successfully
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1"
      timeout: 5s
    hostname: clickhouse
    image: "clickhouse/clickhouse-server:${CLICKHOUSE_VERSION:-25.1.5}"
    ports:
      - "8123:8123"
      - "9000:9000"
    user: "101:101"
  cockroachdb:
    command: [bash, -c, /tmp/scripts/init.sh]
    container_name: cockroachdb-data
    depends_on:
      cockroachdb-server:
        condition: service_healthy
    image: "cockroachdb/cockroach:${COCKROACHDB_VERSION:-v24.3.6}"
    volumes:
      - "./data/cockroachdb/init.sh:/tmp/scripts/init.sh"
      - "${COCKROACHDB_DATA:-./data/cockroachdb/data}:/tmp/data"
  cockroachdb-server:
    command: [start-single-node, --insecure]
    container_name: cockroachdb
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD-SHELL, "curl --fail http://localhost:8080/ || exit 1"]
      timeout: 5s
    image: "cockroachdb/cockroach:${COCKROACHDB_VERSION:-v24.3.6}"
    ports:
      - "26257:26257"
      - "8080:8080"
  confluent-schema-registry:
    container_name: confluent-schema-registry
    depends_on:
      kafka-server:
        condition: service_healthy
    env_file: data/confluent-schema-registry/env/docker.env
    healthcheck:
      interval: 10s
      retries: 5
      test: nc -z confluent-schema-registry 8081
      timeout: 5s
    hostname: confluent-schema-registry
    image: confluentinc/cp-schema-registry:${CONFLUENT_SCHEMA_REGISTRY_VERSION:-7.4.0}
    ports:
      - "8081:8081"
  dagster:
    container_name: dagster
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: [dagster-webserver, -h, 0.0.0.0, -p, "3000", -w, /opt/dagster/app/workspace.yaml]
    environment:
      - DAGSTER_POSTGRES_HOST=postgres
      - "DAGSTER_POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "DAGSTER_POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
      - DAGSTER_POSTGRES_DB=dagster
      - DAGSTER_HOME=/opt/dagster/dagster_home/
    image: "dagster/dagster-k8s:${DAGSTER_VERSION:-1.10.2}"
    ports:
      - "3000:3000"
    volumes:
      - "./data/dagster:/opt/dagster/app/"
  data-caterer:
    container_name: data-caterer
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - DEPLOY_MODE=standalone
    image: "datacatering/data-caterer:${DATA_CATERER_VERSION:-0.15.2}"
    ports:
      - "9898:9898"
    volumes:
      - "./data/data-caterer/connection:/opt/DataCaterer/connection"
      - "./data/data-caterer/plan:/opt/DataCaterer/plan"
  datahub:
    container_name: datahub
    depends_on:
      datahub-gms:
        condition: service_healthy
    env_file: data/datahub-frontend/env/docker.env
    environment:
      - ELASTIC_CLIENT_USERNAME=elastic
      - "ELASTIC_CLIENT_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    hostname: datahub
    image: acryldata/datahub-frontend-react:${DATAHUB_VERSION:-head}
    ports:
      - "9002:9002"
  datahub-actions:
    container_name: datahub-actions
    depends_on:
      datahub-gms:
        condition: service_healthy
    env_file: data/datahub-actions/env/docker.env
    environment:
      - ACTIONS_EXTRA_PACKAGES=${ACTIONS_EXTRA_PACKAGES:-}
      - ACTIONS_CONFIG=${ACTIONS_CONFIG:-}
    hostname: actions
    image: acryldata/datahub-actions:${DATAHUB_VERSION:-head}
  datahub-gms:
    container_name: datahub-gms
    depends_on:
      datahub-upgrade:
        condition: service_completed_successfully
    env_file: data/datahub-upgrade/env/docker-without-neo4j.env
    environment:
      - KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR=${KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR:-true}
      - EBEAN_DATASOURCE_USERNAME=root
      - "EBEAN_DATASOURCE_PASSWORD=${MYSQL_PASSWORD:-root}"
      - ELASTICSEARCH_USERNAME=elastic
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    healthcheck:
      interval: 1s
      retries: 3
      start_period: 90s
      test: "curl -sS --fail http://datahub-gms:8080/health"
      timeout: 5s
    hostname: datahub-gms
    image: acryldata/datahub-gms:${DATAHUB_VERSION:-head}
    ports:
      - "8080:8080"
  datahub-kafka-setup:
    container_name: datahub-kafka-setup
    depends_on:
      confluent-schema-registry:
        condition: service_healthy
      kafka-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /tmp/scripts/init.sh]
    environment:
      - "KAFKA_TOPICS=${KAFKA_TOPICS:-MetadataAuditEvent_v4,MetadataChangeEvent_v4,FailedMetadataChangeEvent_v4,MetadataChangeLog_Versioned_v1,MetadataChangeLog_Timeseries_v1,MetadataChangeProposal_v1,FailedMetadataChangeProposal_v1,PlatformEvent_v1,DataHubUpgradeHistory_v1}"
    image: "confluentinc/confluent-local:${KAFKA_VERSION:-7.9.0}"
    volumes:
      - "./data/kafka/init.sh:/tmp/scripts/init.sh"
  datahub-upgrade:
    command:
      - -u
      - SystemUpdate
    container_name: datahub-upgrade
    depends_on:
      datahub-kafka-setup:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_completed_successfully
      mysql:
        condition: service_completed_successfully
      neo4j:
        condition: service_healthy
    env_file: data/datahub-upgrade/env/docker-without-neo4j.env
    environment:
      - EBEAN_DATASOURCE_USERNAME=root
      - "EBEAN_DATASOURCE_PASSWORD=${MYSQL_PASSWORD:-root}"
      - ELASTICSEARCH_USERNAME=elastic
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    hostname: datahub-upgrade
    image: acryldata/datahub-upgrade:${DATAHUB_VERSION:-head}
    labels:
      datahub_setup_job: true
  debezium:
    container_name: debezium
    depends_on:
      debezium-connect:
        condition: service_healthy
    environment:
      - "KAFKA_CONNECT_URIS=http://debezium-connect:8083"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8080"]
      timeout: 10s
    image: "debezium/debezium-ui:${DEBEZIUM_VERSION:-2.1.2.Final}"
    ports:
      - "8080:8080"
  debezium-connect:
    container_name: debezium-connect
    depends_on:
      - kafka
    environment:
      - "BOOTSTRAP_SERVERS=kafka:29092"
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_connect_statuses
      - KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_REST_ADVERTISED_HOST_NAME=debezium-connect
      - CONNECT_REST_PORT=8083
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8083"]
      timeout: 10s
    image: "debezium/connect:${DEBEZIUM_CONNECT_VERSION:-2.6.2.Final}"
    ports:
      - "8083:8083"
  doris:
    container_name: doris
    depends_on:
      postgres:
        condition: service_completed_successfully
    image: "apache/doris:${DORIS_VERSION:-doris-all-in-one-2.1.0}"
    ports:
      - "8030:8030"
      - "8040:8040"
      - "9030:9030"
  druid:
    command: [router]
    container_name: druid
    depends_on:
      druid-broker:
        condition: service_healthy
      druid-coordinator:
        condition: service_healthy
      druid-historical:
        condition: service_healthy
      druid-middlemanager:
        condition: service_healthy
      postgres:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
    env_file: data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8888/status/health || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.0}"
    ports:
      - "8888:8888"
  druid-broker:
    command: [broker]
    container_name: druid-broker
    depends_on:
      druid-coordinator:
        condition: service_healthy
      postgres:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
    env_file: data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8082/druid/broker/v1/loadstatus || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.0}"
    ports:
      - "8082:8082"
  druid-coordinator:
    command: [coordinator]
    container_name: druid-coordinator
    depends_on:
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8081/status/health || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.0}"
    ports:
      - "8081:8081"
  druid-historical:
    command: [historical]
    container_name: druid-historical
    depends_on:
      druid-coordinator:
        condition: service_healthy
      postgres:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
    env_file: data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8083/druid/historical/v1/readiness || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.0}"
    ports:
      - "8083:8083"
  druid-middlemanager:
    command: [middleManager]
    container_name: druid-middlemanager
    depends_on:
      druid-coordinator:
        condition: service_healthy
      postgres:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
    env_file: data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8091/status/health || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.0}"
    ports:
      - "8091:8091"
      - "8100-8105:8100-8105"
  duckdb:
    container_name: duckdb
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: [tail, -F, anything]
    image: "datacatering/duckdb:${DUCKDB_VERSION:-v1.2.0}"
    volumes:
      - "./data/duckdb:/opt/data"
  elasticsearch:
    container_name: elasticsearch-data
    depends_on:
      elasticsearch-server:
        condition: service_healthy
    entrypoint: /tmp/entrypoint.sh
    environment:
      - "ELASTIC_PASSWORD=${ELASTIC_PASSWORD:-elasticsearch}"
      - "LOGSTASH_INTERNAL_PASSWORD=${LOGSTASH_INTERNAL_PASSWORD:-password}"
      - "KIBANA_SYSTEM_PASSWORD=${KIBANA_SYSTEM_PASSWORD:-password}"
      - "METRICBEAT_INTERNAL_PASSWORD=${METRICBEAT_INTERNAL_PASSWORD:-}"
      - "FILEBEAT_INTERNAL_PASSWORD=${FILEBEAT_INTERNAL_PASSWORD:-}"
      - "HEARTBEAT_INTERNAL_PASSWORD=${HEARTBEAT_INTERNAL_PASSWORD:-}"
      - "MONITORING_INTERNAL_PASSWORD=${MONITORING_INTERNAL_PASSWORD:-}"
      - "BEATS_SYSTEM_PASSWORD=${BEATS_SYSTEM_PASSWORD:-}"
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_VERSION:-8.17.2}"
    volumes:
      - "./data/elasticsearch/data/entrypoint.sh:/tmp/entrypoint.sh"
      - "./data/elasticsearch/data/lib.sh:/tmp/lib.sh"
      - "./data/elasticsearch/data/roles:/tmp/roles"
  elasticsearch-server:
    container_name: elasticsearch
    environment:
      - node.name=elasticsearch
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - "ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
      - XPACK_SECURITY_ENABLED=true
      - discovery.type=single-node
    healthcheck:
      interval: 10s
      retries: 5
      test: "curl -sS --fail http://elasticsearch:9200/_cluster/health?wait_for_status=yellow&timeout=0s"
      timeout: 5s
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_VERSION:-8.17.2}"
    ports:
      - "9200:9200"
      - "9300:9300"
    restart: unless-stopped
    volumes:
      - "./data/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro,Z"
  evidence:
    container_name: evidence
    image: "evidencedev/devenv:${EVIDENCE_VERSION:-latest}"
    init: true
    ports:
      - "3000:3000"
    volumes:
      - "./data/evidence/persist:/evidence-workspace"
  flight-sql:
    command: [tail, -f, /dev/null]
    container_name: flight-sql
    depends_on:
      - duckdb
      - sqlite
    environment:
      - TLS_ENABLED=1
      - "FLIGHT_PASSWORD=${FLIGHT_SQL_PASSWORD:-flight_password}"
      - PRINT_QUERIES=1
    image: "voltrondata/flight-sql:${FLIGHT_SQL_VERSION:-v1.4.1}"
    ports:
      - "31337:31337"
  flink:
    command: taskmanager
    container_name: flink
    depends_on:
      - flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - "FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager"
    expose:
      - 6121
      - 6122
    image: "flink:${FLINK_VERSION:-1.20.1-scala_2.12-java17}"
    links: []
  flink-jobmanager:
    command: jobmanager
    container_name: flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - "FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager"
    expose:
      - 6123
    image: "flink:${FLINK_VERSION:-1.20.1-scala_2.12-java17}"
    ports:
      - "8081:8081"
  fluentd:
    container_name: fluentd
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
    image: "datacatering/fluentd-elasticsearch:${FLUENTD_VERSION:-v1.17.0-debian-1.0}"
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    volumes:
      - "./data/fluentd/etc/fluent.conf:/fluentd/etc/fluent.conf"
  httpbin:
    container_name: http
    environment:
      - "GUNICORN_CMD_ARGS=--capture-output --error-logfile - --access-logfile - --access-logformat '%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s}'"
    image: "kennethreitz/httpbin:${HTTPBIN_VERSION:-latest}"
    ports:
      - "80:80"
  httpd:
    container_name: httpd
    depends_on:
      - fluentd
    image: "httpd:${HTTPD_VERSION:-2.4.63}"
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        fluentd-async-connect: "true"
        tag: httpd.access
    ports:
      - "80:80"
  jupyter:
    command: [jupyter, notebook, --no-browser, "--NotebookApp.token=''", "--NotebookApp.password=''"]
    container_name: jupyter
    image: "quay.io/jupyter/minimal-notebook:${JUPYTER_VERSION:-2024-07-02}"
    ports:
      - "8888:8888"
  kafka:
    container_name: kafka-data
    depends_on:
      kafka-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /tmp/scripts/init.sh]
    environment:
      - "KAFKA_TOPICS=${KAFKA_TOPICS:-accounts,transactions}"
    image: "confluentinc/confluent-local:${KAFKA_VERSION:-7.9.0}"
    volumes:
      - "./data/kafka/init.sh:/tmp/scripts/init.sh"
  kafka-server:
    container_name: kafka
    environment:
      - "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - "KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092"
      - "KAFKA_LISTENERS=PLAINTEXT://kafka:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092"
    expose:
      - 29092
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD-SHELL, /bin/sh, -c, kafka-topics, --bootstrap-server, "kafka:29092", --list]
      timeout: 5s
    image: "confluentinc/confluent-local:${KAFKA_VERSION:-7.9.0}"
    ports:
      - "9092:9092"
  keycloak:
    command: [start-dev, --import-realm]
    container_name: keycloak
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - KC_DB=postgres
      - "KC_DB_USERNAME=${POSTGRES_USER:-postgres}"
      - "KC_DB_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
      - "KC_DB_URL=jdbc:postgresql://postgres:5432/keycloak"
      - KC_REALM_NAME=myrealm
      - "KEYCLOAK_ADMIN=${KEYCLOAK_USER:-admin}"
      - "KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_PASSWORD:-admin}"
    image: "quay.io/keycloak/keycloak:${KEYCLOACK_VERSION:-26.1.3}"
    ports:
      - "8082:8080"
    restart: unless-stopped
    volumes:
      - "./data/keycloak/realm.json:/opt/keycloak/data/import/realm.json:ro"
  kibana:
    container_name: kibana
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - "KIBANA_SYSTEM_PASSWORD=${KIBANA_SYSTEM_PASSWORD:-password}"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:5601/api/status"]
      timeout: 5s
    image: "docker.elastic.co/kibana/kibana:${KIBANA_VERSION:-8.17.2}"
    ports:
      - "5601:5601"
    restart: always
    volumes:
      - "./data/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml"

  kong:
    container_name: kong
    depends_on:
      kong-data:
        condition: service_completed_successfully
      postgres:
        condition: service_completed_successfully
    environment:
      - KONG_DATABASE=postgres
      - KONG_PG_HOST=postgres
      - KONG_PG_USER=postgres
      - KONG_PG_PASSWORD=postgres
      - KONG_PROXY_ACCESS_LOG=/dev/stdout
      - KONG_ADMIN_ACCESS_LOG=/dev/stdout
      - KONG_PROXY_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_LISTEN=0.0.0.0:8001,0.0.0.0:8444 ssl
      - KONG_PLUGINS=rate-limiting
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD-SHELL, kong health]
      timeout: 10s
    image: "kong:${KONG_VERSION:-3.9.0}"
    ports:
      - 8001:8001
      - 8000:8000
    restart: unless-stopped
  kong-data:
    command:
      - kong
      - migrations
      - bootstrap
    container_name: kong-data
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - KONG_DATABASE=postgres
      - KONG_PG_HOST=postgres
      - KONG_PG_USER=postgres
      - KONG_PG_PASSWORD=postgres
    image: "kong:${KONG_VERSION:-3.9.0}"
  logstash:
    container_name: logstash
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
    environment:
      LOGSTASH_INTERNAL_PASSWORD: "${LOGSTASH_INTERNAL_PASSWORD:-password}"
      LS_JAVA_OPTS: -Xms256m -Xmx256m
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:9600"]
      timeout: 5s
    image: "docker.elastic.co/logstash/logstash:${LOGSTASH_VERSION:-8.17.2}"
    ports:
      - "5044:5044"
      - "50000:50000/tcp"
      - "50000:50000/udp"
      - "9600:9600"
    restart: unless-stopped
    volumes:
      - "./data/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml"
      - "./data/logstash/pipeline:/usr/share/logstash/pipeline"
  maestro:
    container_name: maestro
    depends_on:
      cockroachdb:
        condition: service_completed_successfully
    environment:
      - CONDUCTOR_CONFIGS_JDBCURL=jdbc:postgresql://cockroachdb:26257/maestro
      - CONDUCTOR_CONFIGS_JDBCUSERNAME=root
    image: "datacatering/maestro:${MAESTRO_VERSION:-0.1.0}"
    ports:
      - "8081:8080"
  mage-ai:
    command: mage start your_first_project
    container_name: mage-ai
    environment:
      - USER_CODE_PATH=/home/src/your_first_project
    image: "mageai/mageai:${MAGE_AI_VERSION:-0.9.75}"
    ports:
      - "6789:6789"
    restart: on-failure
  mariadb:
    container_name: mariadb
    environment:
      - "MARIADB_USER=${MARIADB_USER:-user}"
      - "MARIADB_PASSWORD=${MARIADB_PASSWORD:-password}"
      - MARIADB_ROOT_PASSWORD=root
      - MARIADB_DATABASE=customer
    image: "mariadb:${MARIADB_VERSION:-11.7.2}"
    ports:
      - "3306:3306"
    restart: always
  marquez:
    container_name: marquez-web
    depends_on:
      - marquez-data
    environment:
      - MARQUEZ_HOST=host.docker.internal
      - MARQUEZ_PORT=5002
    image: "marquezproject/marquez-web:${MARQUEZ_VERSION:-0.50.0}"
    ports:
      - "3001:3000"
  marquez-data:
    command: [-c, /tmp/scripts/init.sh]
    container_name: marquez-data
    depends_on:
      marquez-server:
        condition: service_healthy
    entrypoint: /bin/bash
    environment:
      - "MARQUEZ_URL=http://marquez:5000"
    image: "marquezproject/marquez:${MARQUEZ_VERSION:-0.50.0}"
    volumes:
      - "./data/marquez/init.sh:/tmp/scripts/init.sh"
      - "${MARQUEZ_DATA:-./data/marquez/data}:/tmp/data"
  marquez-server:
    container_name: marquez
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - MARQUEZ_CONFIG=/opt/app/marquez.yaml
      - MARQUEZ_PORT=5000
      - MARQUEZ_ADMIN_PORT=5001
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=marquez
      - "POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:5001/healthcheck"]
      timeout: 5s
    image: "marquezproject/marquez:${MARQUEZ_VERSION:-0.50.0}"
    ports:
      - "5002:5000"
      - "5001:5001"
    volumes:
      - "./data/marquez/conf:/opt/app"
  metabase:
    container_name: metabase
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      MB_DB_DBNAME: metabase
      MB_DB_HOST: postgres
      MB_DB_PASS: postgres
      MB_DB_PORT: 5432
      MB_DB_TYPE: postgres
      MB_DB_USER: postgres
    healthcheck:
      interval: 15s
      retries: 5
      test: [CMD-SHELL, curl, --fail, -I, http://localhost:3000/api/health || exit 1]
      timeout: 5s
    image: "metabase/metabase:${METABASE_VERSION:-v0.53.4}"
    ports:
      - "3000:3000"
    volumes:
      - /dev/urandom:/dev/random:ro
  minio:
    command: [server, /data, --console-address, ":9001"]
    container_name: minio
    environment:
      - "MINIO_ROOT_USER=${MINIO_USER:-minioadmin}"
      - "MINIO_ROOT_PASSWORD=${MINIO_PASSWORD:-minioadmin}"
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, mc, ready, local]
      timeout: 5s
    image: "quay.io/minio/minio:${MINIO_VERSION:-RELEASE.2024-06-04T19-20-08Z}"
    ports:
      - "9000:9000"
      - "9001:9001"
  mongodb:
    command: [/bin/sh, -c, /opt/app/my_data.sh]
    container_name: mongodb-connect
    depends_on:
      - mongodb-server
    environment:
      - "CONN_STR=mongodb://${MONGODB_USER:-user}:${MONGODB_PASSWORD:-password}@mongodb-server"
    image: "mongodb/mongodb-community-server:${MONGODB_VERSION:-8.0.5-ubi8}"
    volumes:
      - "./data/mongodb:/opt/app"
  mongodb-server:
    container_name: mongodb
    environment:
      - "MONGO_INITDB_ROOT_USERNAME=${MONGODB_USER:-user}"
      - "MONGO_INITDB_ROOT_PASSWORD=${MONGODB_PASSWORD:-password}"
    image: "mongo:${MONGODB_VERSION:-8.0.5}"
    ports:
      - "27017:27017"
  mssql:
    container_name: mssql
    environment:
      - "SA_PASSWORD=${MSSQL_PASSWORD:-yourStrong(!)Password}"
      - ACCEPT_EULA=Y
    healthcheck:
      interval: 10s
      retries: 10
      test: [CMD-SHELL, mssql-health-check]
      timeout: 10s
    image: "mcr.microsoft.com/mssql/server:${MSSQL_VERSION:-2022-latest}"
    ports:
      - "1433:1433"
    volumes:
      - "./data/mssql/mssql-health-check:/usr/local/bin/mssql-health-check"
  mysql:
    command: [/bin/bash, -c, /tmp/scripts/init.sh]
    container_name: mysql-data
    depends_on:
      mysql-server:
        condition: service_healthy
    environment:
      - "MYSQL_PASSWORD=${MYSQL_PASSWORD:-root}"
    image: "mysql:${MYSQL_VERSION:-9.2.0}"
    volumes:
      - "./data/mysql/init.sh:/tmp/scripts/init.sh"
      - "${MYSQL_DATA:-./data/mysql/data}:/tmp/data"
  mysql-server:
    container_name: mysql
    environment:
      - "MYSQL_ROOT_PASSWORD=${MYSQL_PASSWORD:-root}"
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, mysqladmin, ping, -h, localhost, -u, root, -p$$MYSQL_ROOT_PASSWORD]
      timeout: 5s
    image: "mysql:${MYSQL_VERSION:-9.2.0}"
    ports:
      - "3306:3306"
  neo4j:
    container_name: neo4j
    environment:
      - NEO4J_AUTH=none
    healthcheck:
      interval: 30s
      retries: 5
      test: [CMD-SHELL, "cypher-shell -u neo4j -p test 'RETURN 1' || exit 1"]
      timeout: 10s
    image: "neo4j:${NEO4J_VERSION:-2025.01.0}"
    ports:
      - "7474:7474"
      - "7687:7687"
  openmetadata:
    command:
      - /opt/airflow/ingestion_dependency.sh
    container_name: openmetadata-ingestion
    depends_on:
      openmetadata-server:
        condition: service_healthy
    entrypoint: /bin/bash
    env_file: ./data/openmetadata-ingestion/env/docker.env
    environment:
      - "DB_USER=${MYSQL_USER:-root}"
      - "DB_USER_PASSWORD=${MYSQL_PASSWORD:-root}"
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 30s
      test: [CMD, curl, --fail, "http://localhost:8080/health"]
      timeout: 10s
    image: "docker.getcollate.io/openmetadata/ingestion:${OPENMETADATA_VERSION:-1.6.5}"
    ports:
      - "8080:8080"
  openmetadata-server:
    container_name: openmetadata
    depends_on:
      openmetadata-setup:
        condition: service_completed_successfully
    env_file: ./data/openmetadata/env/docker.env
    environment:
      - "DB_USER=${MYSQL_USER:-root}"
      - "DB_USER_PASSWORD=${MYSQL_PASSWORD:-root}"
      - "ELASTICSEARCH_USER=${ELASTICSEARCH_USER:-elastic}"
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD, wget, -q, --spider,  "http://localhost:8586/healthcheck"]
      timeout: 5s
    image: "docker.getcollate.io/openmetadata/server:${OPENMETADATA_VERSION:-1.6.4}"
    ports:
      - "8585:8585"
      - "8586:8586"
    restart: always
  openmetadata-setup:
    command: ./bootstrap/openmetadata-ops.sh migrate
    container_name: openmetadata-setup
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
      mysql:
        condition: service_completed_successfully
    env_file: ./data/openmetadata/env/docker.env
    environment:
      - "DB_USER=${MYSQL_USER:-root}"
      - "DB_USER_PASSWORD=${MYSQL_PASSWORD:-root}"
      - "ELASTICSEARCH_USER=${ELASTICSEARCH_USER:-elastic}"
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    image: "docker.getcollate.io/openmetadata/server:${OPENMETADATA_VERSION:-1.6.4}"
  opensearch:
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - "OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_PASSWORD:-!BigData#1}"
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD, curl, --fail, "https://localhost:9200", -ku, "admin:${OPENSEARCH_PASSWORD:-!BigData#1}"]
      timeout: 5s
    image: "opensearchproject/opensearch:${OPENSEARCH_VERSION:-2.19.1}"
    ports:
      - "9600:9600"
      - "9200:9200"
  pinot:
    command: "StartServer -zkAddress zookeeper:2181"
    container_name: pinot-server
    depends_on:
      pinot-broker:
        condition: service_healthy
    environment:
      - "JAVA_OPTS=-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx16G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-server.log"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8098/health/readiness"]
      timeout: 5s
    image: "apachepinot/pinot:${PINOT_VERSION:-1.2.0}"
    ports:
      - "8098:8098"
    restart: unless-stopped
  pinot-broker:
    command: "StartBroker -zkAddress zookeeper:2181"
    container_name: pinot-broker
    depends_on:
      pinot-controller:
        condition: service_healthy
    environment:
      - "JAVA_OPTS=-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-broker.log"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8099/health"]
      timeout: 5s
    image: "apachepinot/pinot:${PINOT_VERSION:-1.2.0}"
    ports:
      - "8099:8099"
    restart: unless-stopped
  pinot-controller:
    command: "StartController -zkAddress zookeeper:2181"
    container_name: pinot
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      - "JAVA_OPTS=-Dplugins.dir=/opt/pinot/plugins -Xms1G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-controller.log"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:9000/pinot-controller/admin"]
      timeout: 5s
    image: "apachepinot/pinot:${PINOT_VERSION:-1.2.0}"
    ports:
      - "9000:9000"
    restart: unless-stopped
  polaris:
    container_name: polaris
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD, curl, "http://localhost:8182/healthcheck"]
      timeout: 10s
    image: "datacatering/polaris:${POLARIS_VERSION:-1.0.0}"
    ports:
      - "8181:8181"
      - "8182:8182"
  postgres:
    command: [/bin/bash, -c, /tmp/scripts/init.sh]
    container_name: postgres-data
    depends_on:
      postgres-server:
        condition: service_healthy
    environment:
      - "POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "PGPASSWORD=${POSTGRES_PASSWORD:-postgres}"
    image: "postgres:${POSTGRES_VERSION:-17.4}"
    volumes:
      - "./data/postgres/init.sh:/tmp/scripts/init.sh"
      - "${POSTGRES_DATA:-./data/postgres/data}:/tmp/data"
  postgres-server:
    container_name: postgres
    environment:
      - "POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
      - PGDATA=/data/postgres
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, pg_isready]
      timeout: 5s
    image: "postgres:${POSTGRES_VERSION:-17.4}"
    ports:
      - "5432:5432"
  prefect:
    container_name: prefect-data
    depends_on:
      - prefect-server
    entrypoint: [/opt/prefect/app/start_flows.sh]
    environment:
      - "PREFECT_API_URL=http://host.docker.internal:4200/api"
    image: "prefecthq/prefect:${PREFECT_VERSION:-3.2.8-python3.11}"
    volumes:
      - "./data/prefect/flows:/root/flows"
      - "./data/prefect/start_flows.sh:/opt/prefect/app/start_flows.sh"
    working_dir: /root/flows
  prefect-server:
    container_name: prefect
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: [/opt/prefect/entrypoint.sh, prefect, server, start]
    environment:
      - "PREFECT_UI_URL=http://127.0.0.1:4200/api"
      - "PREFECT_API_URL=http://127.0.0.1:4200/api"
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - "PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/prefect"
    image: "prefecthq/prefect:${PREFECT_VERSION:-3.2.8-python3.11}"
    ports:
      - "4200:4200"
    restart: always
  presto:
    container_name: presto
    depends_on:
      postgres:
        condition: service_completed_successfully
    image: "prestodb/presto:${PRESTO_VERSION:-0.291}"
    ports:
      - "8083:8080"
    volumes:
      - "./data/presto/etc:/opt/presto-server/etc"
      - "./data/presto/catalog:/opt/presto-server/etc/catalog"
  rabbitmq:
    container_name: rabbitmq
    environment:
      - "RABBITMQ_DEFAULT_USER=${RABBITMQ_USER:-guest}"
      - "RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD:-guest}"
    healthcheck:
      interval: 30s
      retries: 3
      test: rabbitmq-diagnostics -q ping
      timeout: 30s
    hostname: my-rabbit
    image: "rabbitmq:${RABBITMQ_VERSION:-4.0.7-management}"
    ports:
      - "5672:5672"
      - "15672:15672"
  redash:
    command: scheduler
    container_name: redash-scheduler
    depends_on:
      postgres:
        condition: service_completed_successfully
      redash-server:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      PYTHONUNBUFFERED: 0
      QUEUES: "queries,scheduled_queries,celery"
      REDASH_COOKIE_SECRET: veryverysecret
      REDASH_DATABASE_URL: "postgresql://postgres:postgres@postgres/redash"
      REDASH_LOG_LEVEL: INFO
      REDASH_REDIS_URL: "redis://redis:6379/0"
      WORKERS_COUNT: 1
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD, /bin/sh, -c, /app/bin/docker-entrypoint workers_healthcheck]
      timeout: 5s
    image: "redash/redash:${REDASH_VERSION:-10.1.0.b50633}"
  redash-data:
    command: [/app/bin/docker-entrypoint, create_db]
    container_name: redash-data
    environment:
      PYTHONUNBUFFERED: 0
      REDASH_COOKIE_SECRET: veryverysecret
      REDASH_DATABASE_URL: "postgresql://postgres:postgres@postgres/redash"
      REDASH_LOG_LEVEL: INFO
      REDASH_REDIS_URL: "redis://redis:6379/0"
    image: "redash/redash:${REDASH_VERSION:-10.1.0.b50633}"
  redash-server:
    command: server
    container_name: redash
    depends_on:
      redash-data:
        condition: service_completed_successfully
    environment:
      PYTHONUNBUFFERED: 0
      REDASH_COOKIE_SECRET: veryverysecret
      REDASH_DATABASE_URL: "postgresql://postgres:postgres@postgres/redash"
      REDASH_LOG_LEVEL: INFO
      REDASH_REDIS_URL: "redis://redis:6379/0"
      REDASH_WEB_WORKERS: 4
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD, curl, --output, /dev/null, --silent, --head, --fail, http://localhost:5000/ping]
      timeout: 5s
    image: "redash/redash:${REDASH_VERSION:-10.1.0.b50633}"
    ports:
      - "5000:5000"
  redis:
    container_name: redis
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, redis-cli, --raw, incr, ping]
      timeout: 5s
    image: "redis:${REDIS_VERSION:-7.4.2}"
    ports:
      - "6379:6379"
  solace:
    container_name: solace-data
    depends_on:
      solace-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /opt/app/my_data.sh]
    image: "solace/solace-pubsub-standard:${SOLACE_VERSION:-10.11}"
    volumes:
      - "./data/solace:/opt/app"
  solace-server:
    container_name: solace
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 1
    environment:
      - username_admin_globalaccesslevel=admin
      - "username_admin_password=${SOLACE_PASSWORD:-admin}"
      - system_scaling_maxconnectioncount=100
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD, curl, --output, /dev/null, --silent, --head, --fail, "http://localhost:8080"]
      timeout: 5s
    image: "solace/solace-pubsub-standard:${SOLACE_VERSION:-10.11}"
    ports:
      - "8080:8080"
      - "55554:55555"
    shm_size: 1g
    ulimits:
      core: -1
      nofile:
        hard: 6592
        soft: 2448
  sonarqube:
    container_name: sonarqube
    environment:
      - SONAR_WEB_CONTEXT=/sonar
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, "wget --spider localhost:9000/sonar"]      
      timeout: 10s
    image: "sonarqube:${SONARQUBE_VERSION:-lts-community}"
    ports:
      - "9000:9000"
  spanner:
    container_name: spanner
    image: "gcr.io/cloud-spanner-emulator/emulator:${SPANNER_VERSION:-1.5.29}"
    ports:
      - "9010:9010"
      - "9020:9020"
  sqlite:
    command: [tail, -f, /dev/null]
    container_name: sqlite
    image: "keinos/sqlite3:${SQLITE_VERSION:-3.49.0}"
    volumes:
      - "./data/sqlite:/opt/data"
  superset:
    command: [/app/docker/docker-init.sh]
    container_name: superset_init
    depends_on:
      postgres:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      superset-server:
        condition: service_healthy
    env_file:
      - ./data/superset/docker/.env
    healthcheck:
      disable: true
    image: "apache/superset:${SUPERSET_VERSION:-4.1.1}"
    user: root
    volumes:
      - "./data/superset/docker:/app/docker"
  superset-server:
    command: [/app/docker/docker-bootstrap.sh, app-gunicorn]
    container_name: superset
    env_file:
      - ./data/superset/docker/.env
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, curl, --output, /dev/null, --silent, --head, --fail, "http://localhost:8088/health"]
      timeout: 5s
    image: "apache/superset:${SUPERSET_VERSION:-4.1.1}"
    ports:
      - "8088:8088"
    user: root
    volumes:
      - "./data/superset/docker:/app/docker"
  temporal:
    command: [server, start-dev, --db-filename, /opt/data/db/temporal.db, --ip, 0.0.0.0, --metrics-port, "9233"]
    container_name: temporal
    entrypoint: temporal
    environment: []
    expose:
      - 8233
      - 7233
    image: "temporalio/server:${TEMPORAL_VERSION:-1.27.1.0}"
    ports:
      - "8233:8233"
      - "7233:7233"
      - "9233:9233"
  trino:
    container_name: trino
    depends_on:
      postgres:
        condition: service_completed_successfully
    image: "trinodb/trino:${TRINO_VERSION:-471}"
    ports:
      - "8084:8080"
    volumes:
      - "./data/trino/etc:/usr/lib/trino/etc:ro"
      - "./data/trino/catalog:/etc/trino/catalog"
  unitycatalog:
    container_name: unitycatalog
    image: "datacatering/unitycatalog:${UNITYCATALOG_VERSION:-0.1.0}"
    ports:
      - "8081:8081"
  zookeeper:
    container_name: zookeeper
    environment:
      - ZOO_MY_ID=1
    healthcheck:
      interval: 5s
      retries: 3
      test: "nc -z localhost 2181 || exit -1"
      timeout: 5s
    image: "zookeeper:${ZOOKEEPER_VERSION:-3.9.3}"
    ports:
      - "2181:2181"
