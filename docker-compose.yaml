version: "3.9"
services:
  #database
  cassandra:
    container_name: "cassandra"
    image: "datacatering/dse-server:6.8.48"
    environment:
      - "DS_LICENSE=accept"
    volumes:
      - "./data/cassandra/persist:/var/lib/cassandra"
      - "./data/cassandra/my_data.cql:/docker-entrypoint-initdb.d/my_data.cql"
    healthcheck:
      test: [ "CMD-SHELL", "[ $$(nodetool statusgossip) = running ]" ]
      interval: "30s"
      timeout: "10s"
      retries: 3
    ports:
      - "9042:9042"
    cap_add:
      - "IPC_LOCK"
    ulimits:
      memlock: -1

  elasticsearch:
    container_name: "elasticsearch"
    image: "docker.elastic.co/elasticsearch/elasticsearch:8.13.4"
    environment:
      - "node.name=elasticsearch"
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "ELASTIC_PASSWORD=elasticsearch"
      - "discovery.type=single-node"
    volumes:
      - "./data/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro,Z"
      - "./data/elasticsearch/persist:/usr/share/elasticsearch/data:Z"
    ports:
      - "9200:9200"
      - "9300:9300"
    restart: "unless-stopped"

  mariadb:
    container_name: "mariadb"
    image: "mariadb:11.4.2"
    environment:
      - "MARIADB_USER=user"
      - "MARIADB_PASSWORD=password"
      - "MARIADB_ROOT_PASSWORD=root"
      - "MARIADB_DATABASE=customer"
    volumes:
      - "./data/mariadb/persist:/var/lib/mysql:Z"
      - "./data/mariadb:/opt/app"
    ports:
      - "3306:3306"
    restart: "always"

  mongodb-server:
    container_name: "mongodb"
    image: "mongo:7.0.11"
    environment:
      - "MONGO_INITDB_ROOT_USERNAME=user"
      - "MONGO_INITDB_ROOT_PASSWORD=password"
    volumes:
      - "./data/mongodb/persist:/data/db"
    ports:
      - "27017:27017"

  mongodb:
    container_name: "mongodb-connect"
    image: "mongodb/mongodb-community-server:7.0.11-ubi8"
    environment:
      - "CONN_STR=mongodb://user:password@mongodb-server"
    volumes:
      - "./data/mongodb:/opt/app"
    command: [ "/bin/sh", "-c", "/opt/app/my_data.sh" ]
    depends_on:
      - "mongodb-server"

  mysql:
    container_name: "mysql"
    image: "mysql:8.4.0"
    environment:
      - "MYSQL_ROOT_PASSWORD=root"
    command: "--mysql-native-password=ON"
    volumes:
      - "./data/mysql/persist:/var/lib/mysql"
      - "./data/mysql/my_data.sql:/docker-entrypoint-initdb.d/my_data.sql"
    healthcheck:
      test: [ "CMD-SHELL", "mysqladmin" ,"ping", "-h", "localhost", "-u", "root", "-p$$MYSQL_ROOT_PASSWORD" ]
      timeout: "20s"
      retries: 3
    ports:
      - "3306:3306"

  postgres:
    container_name: "postgres"
    image: "postgres:16.3"
    environment:
      - "POSTGRES_USER=postgres"
      - "POSTGRES_PASSWORD=postgres"
      - "PGDATA=/data/postgres"
    volumes:
      - "./data/postgres/persist:/data/postgres"
      - "./data/postgres/my_data.sql:/docker-entrypoint-initdb.d/my_data.sql"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready" ]
      interval: "10s"
      timeout: "5s"
      retries: 3
    ports:
      - "5432:5432"

  #data catalog
  marquez-server:
    container_name: "marquez"
    image: "marquezproject/marquez:0.47.0"
    environment:
      - "MARQUEZ_CONFIG=/opt/app/marquez.yaml"
      - "MARQUEZ_PORT=5000"
      - "MARQUEZ_ADMIN_PORT=5001"
      - "POSTGRES_HOST=postgres"
      - "POSTGRES_PORT=5432"
      - "POSTGRES_DB=marquez"
      - "POSTGRES_USER=postgres"
      - "POSTGRES_PASSWORD=postgres"
    volumes:
      - "./data/marquez/persist:/opt/marquez"
      - "./data/marquez:/opt/app"
    ports:
      - "5002:5000"
      - "5001:5001"
    depends_on:
      postgres:
        condition: "service_healthy"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:5001/healthcheck" ]
      interval: "10s"
      timeout: "5s"
      retries: 3

  marquez-data:
    container_name: "marquez-data"
    image: "marquezproject/marquez:0.47.0"
    entrypoint: "/bin/bash"
    command: [ "-c", "/opt/app/seed.sh" ]
    environment:
      - "MARQUEZ_URL=http://marquez:5000"
    volumes:
      - "./data/marquez:/opt/app"
    depends_on:
      marquez-server:
        condition: "service_healthy"

  marquez:
    container_name: "marquez-web"
    image: "marquezproject/marquez-web:0.47.0"
    environment:
      - "MARQUEZ_HOST=host.docker.internal"
      - "MARQUEZ_PORT=5002"
    ports:
      - "3001:3000"
    depends_on:
      - "marquez-data"

  #http
  httpbin:
    container_name: "http"
    image: "kennethreitz/httpbin:latest"
    environment:
      - "GUNICORN_CMD_ARGS=--capture-output --error-logfile - --access-logfile - --access-logformat '%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s}'"
    ports:
      - "80:80"

  #identity management
  keycloak:
    container_name: "keycloak"
    image: "quay.io/keycloak/keycloak:24.0.5"
    command: [ "start-dev", "--import-realm" ]
    environment:
      - "KC_DB=postgres"
      - "KC_DB_USERNAME=postgres"
      - "KC_DB_PASSWORD=postgres"
      - "KC_DB_URL=jdbc:postgresql://postgres:5432/keycloak"
      - "KC_REALM_NAME=myrealm"
      - "KEYCLOAK_ADMIN=admin"
      - "KEYCLOAK_ADMIN_PASSWORD=admin"
    ports:
      - "8082:8080"
    volumes:
      - "./data/keycloak/realm.json:/opt/keycloak/data/import/realm.json:ro"
    depends_on:
      postgres:
        condition: "service_healthy"
    restart: "unless-stopped"

  #job orchestrator
  airflow:
    container_name: "airflow"
    image: "apache/airflow:2.9.1"
    command: "standalone"
    environment:
      - "AIRFLOW_UID=50000"
      - "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres/airflow"
      - "AIRFLOW__CORE__FERNET_KEY="
      - "AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true"
      - "AIRFLOW__CORE__LOAD_EXAMPLES=true"
      - "AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
    volumes:
      - "./data/airflow/dags:/opt/airflow/dags"
    ports:
      - "8081:8080"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: "30s"
      timeout: "10s"
      retries: 5
      start_period: "30s"
    restart: "always"
    user: "50000:0"
    depends_on:
      airflow-init:
        condition: "service_completed_successfully"

  airflow-init:
    container_name: "airflow-init"
    image: "apache/airflow:2.9.1"
    entrypoint: "/bin/bash"
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "50000:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      - "AIRFLOW_UID=50000"
      - "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres/airflow"
      - "_AIRFLOW_DB_MIGRATE=true"
      - "_AIRFLOW_WWW_USER_CREATE=true"
      - "_AIRFLOW_WWW_USER_USERNAME=airflow"
      - "_AIRFLOW_WWW_USER_PASSWORD=airflow"
      - "_PIP_ADDITIONAL_REQUIREMENTS="
    user: "0:0"
    depends_on:
      postgres:
        condition: "service_healthy"

  dagster:
    container_name: "dagster"
    image: "dagster/dagster-k8s:1.7.8"
    entrypoint: [ "dagster-webserver", "-h", "0.0.0.0", "-p", "3000", "-w", "/opt/dagster/app/workspace.yaml" ]
    environment:
      - "DAGSTER_POSTGRES_HOST=postgres"
      - "DAGSTER_POSTGRES_USER=postgres"
      - "DAGSTER_POSTGRES_PASSWORD=postgres"
      - "DAGSTER_POSTGRES_DB=dagster"
      - "DAGSTER_HOME=/opt/dagster/dagster_home/"
    volumes:
      - "./data/dagster/persist:/opt/dagster/dagster_home/"
      - "./data/dagster:/opt/dagster/app/"
    ports:
      - "3000:3000"
    depends_on:
      postgres:
        condition: "service_healthy"

  mage-ai:
    container_name: "mage-ai"
    image: "mageai/mageai:0.9.71"
    command: "mage start your_first_project"
    environment:
      - "USER_CODE_PATH=/home/src/your_first_project"
    ports:
      - "6789:6789"
    volumes:
      - "./data/mage-ai/persist:/home/src/"
    restart: "on-failure"

  prefect-server:
    container_name: "prefect"
    image: "prefecthq/prefect:2.19.4-python3.11"
    entrypoint: [ "/opt/prefect/entrypoint.sh", "prefect", "server", "start" ]
    environment:
      - "PREFECT_UI_URL=http://127.0.0.1:4200/api"
      - "PREFECT_API_URL=http://127.0.0.1:4200/api"
      - "PREFECT_SERVER_API_HOST=0.0.0.0"
      - "PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/prefect"
    volumes:
      - "./data/prefect/persist:/root/.prefect"
    ports:
      - "4200:4200"
    depends_on:
      postgres:
        condition: "service_healthy"
    restart: "always"

  prefect:
    container_name: "prefect-data"
    image: "prefecthq/prefect:2.19.4-python3.11"
    entrypoint: [ "/opt/prefect/app/start_flows.sh" ]
    environment:
      - "PREFECT_API_URL=http://host.docker.internal:4200/api"
    volumes:
      - "./data/prefect/flows:/root/flows"
      - "./data/prefect/start_flows.sh:/opt/prefect/app/start_flows.sh"
    working_dir: "/root/flows"
    depends_on:
      - "prefect-server"

  #messaging
  activemq:
    container_name: "activemq"
    image: "apache/activemq-artemis:2.34.0"
    environment:
      - "ARTEMIS_USER=artemis"
      - "ARTEMIS_PASSWORD=artemis"
    volumes:
      - "./data/activemq/persist:/var/lib/artemis-instance"
    ports:
      - "61616:61616"
      - "8161:8161"
    healthcheck:
      test: [ "CMD-SHELL", "curl -k -f http://localhost:8161/admin" ]
      interval: "15s"
      timeout: "5s"
      retries: 3

  kafka-server:
    container_name: "kafka"
    image: "confluentinc/confluent-local:7.6.1"
    environment:
      - "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      - "KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT"
      - "KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092"
      - "KAFKA_LISTENERS=PLAINTEXT://kafka:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092"
    volumes:
      - "./data/kafka/persist:/var/lib/kafka/data"
    healthcheck:
      test: [ "CMD-SHELL", "/bin/sh", "-c", "kafka-topics", "--bootstrap-server", "kafka:29092", "--list" ]
      interval: "15s"
      timeout: "5s"
      retries: 3
    ports:
      - "9092:9092"
    expose:
      - "29092"

  kafka:
    container_name: "kafka-data"
    image: "confluentinc/confluent-local:7.6.1"
    entrypoint: [ "/bin/sh", "-c", "/opt/app/my_data.sh" ]
    volumes:
      - "./data/kafka:/opt/app"
    depends_on:
      kafka-server:
        condition: "service_healthy"

  rabbitmq:
    container_name: "rabbitmq"
    image: "rabbitmq:3.13.3-management"
    environment:
      - "RABBITMQ_DEFAULT_USER=guest"
      - "RABBITMQ_DEFAULT_PASS=guest"
    volumes:
      - "./data/rabbitmq/persist:/var/lib/rabbitmq"
    ports:
      - "5672:5672"
      - "15672:15672"
    healthcheck:
      test: "rabbitmq-diagnostics -q ping"
      interval: "30s"
      timeout: "30s"
      retries: 3
    hostname: "my-rabbit"

  solace-server:
    container_name: "solace"
    image: "solace/solace-pubsub-standard:10.8"
    environment:
      - "username_admin_globalaccesslevel=admin"
      - "username_admin_password=admin"
      - "system_scaling_maxconnectioncount=100"
    volumes:
      - "./data/solace/persist:/var/lib/solace"
    healthcheck:
      test: [ "CMD-SHELL", "curl", "--output", "/dev/null", "--silent", "--head", "--fail", "http://localhost:8080" ]
      interval: "30s"
      timeout: "5s"
      retries: 3
    ports:
      - "8080:8080"
      - "55554:55555"
    shm_size: "1g"
    ulimits:
      core: -1
      nofile:
        soft: 2448
        hard: 6592
    deploy:
      restart_policy:
        condition: "on-failure"
        max_attempts: 1

  solace:
    container_name: "solace-data"
    image: "solace/solace-pubsub-standard:10.8"
    entrypoint: [ "/bin/sh", "-c", "/opt/app/my_data.sh" ]
    volumes:
      - "./data/solace:/opt/app"
    depends_on:
      solace-server:
        condition: "service_healthy"

  #object storage
  minio:
    container_name: "minio"
    image: "quay.io/minio/minio:RELEASE.2024-06-04T19-20-08Z"
    command: [ "server", "/data", "--console-address", ":9001" ]
    environment:
      - "MINIO_ROOT_USER=minioadmin"
      - "MINIO_ROOT_PASSWORD=minioadmin"
    volumes:
      - "./data/minio/persist:/data"
    ports:
      - "9000:9000"
      - "9001:9001"
    healthcheck:
      test: [ "CMD", "mc", "ready", "local" ]
      interval: 5s
      timeout: 5s
      retries: 5

  #query engine
  duckdb:
    container_name: "duckdb"
    image: "datacatering/duckdb:v1.0.0"
    entrypoint: [ "tail", "-F", "anything" ]
    volumes:
      - "./data/duckdb:/opt/data"
    depends_on:
      postgres:
        condition: "service_healthy"

  presto:
    container_name: "presto"
    image: "prestodb/presto:0.287"
    volumes:
      - "./data/presto/etc:/opt/presto-server/etc"
      - "./data/presto/catalog:/opt/presto-server/etc/catalog"
    ports:
      - "8083:8080"
    depends_on:
      postgres:
        condition: "service_healthy"

  trino:
    container_name: "trino"
    image: "trinodb/trino:449"
    volumes:
      - "./data/trino/etc:/usr/lib/trino/etc:ro"
      - "./data/trino/catalog:/etc/trino/catalog"
    ports:
      - "8084:8080"
    depends_on:
      postgres:
        condition: "service_healthy"

  #real-time OLAP

  #test data management
  data-caterer:
    container_name: "data-caterer"
    image: "datacatering/data-caterer-basic:0.10.6"
    environment:
      - "DEPLOY_MODE=standalone"
    ports:
      - "9898:9898"

  # Debezium for change data capture
  debezium:
    container_name: "debezium"
    image: "debezium/connect:latest"
    platform: linux/amd64
    depends_on:
      - mariadb
      - redpanda
    environment:
      - "BOOTSTRAP_SERVERS=redpanda:9092"
      - "GROUP_ID=1"
      - "CONFIG_STORAGE_TOPIC=my_connect_configs"
      - "OFFSET_STORAGE_TOPIC=my_connect_offsets"
      - "STATUS_STORAGE_TOPIC=my_connect_statuses"
      - "CONNECT_KEY_CONVERTER=org.apache.kafka.connect.storage.StringConverter"
      - "CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter"
      - "CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=false"
      - "CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter"
      - "CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter"
      - "CONNECT_REST_ADVERTISED_HOST_NAME=debezium"
      - "CONNECT_GROUP_ID=1"
      - "CONNECT_CONFIG_STORAGE_TOPIC=my_connect_configs"
      - "CONNECT_OFFSET_STORAGE_TOPIC=my_connect_offsets"
      - "CONNECT_STATUS_STORAGE_TOPIC=my_connect_statuses"
      - "CONNECT_LOG4J_LOGGERS=org.apache.kafka.connect.runtime.rest=WARN,org.reflections=WARN"
    ports:
      - "8083:8083"
    volumes:
      - "./data/debezium:/kafka/config"

  redpanda:
    container_name: "redpanda"
    image: "docker.redpanda.com/redpandadata/redpanda:v23.3.9"
    command:
      - "redpanda start --overprovisioned --smp 1 --memory 1G --reserve-memory 0M --node-id 0 --check=false --kafka-addr PLAINTEXT://0.0.0.0:9092 --advertise-kafka-addr PLAINTEXT://redpanda:9092"
    ports:
      - "9092:9092"
    volumes:
      - redpanda_data:/var/lib/redpanda/data

  redpanda-console:
    container_name: "redpanda-console"
    image: "docker.redpanda.com/redpandadata/console:latest"
    environment:
      - "KAFKA_BROKERS=redpanda:9092"
      - "SCHEMA_REGISTRY_URL=http://redpanda:8081"
    ports:
      - "8080:8080"
    depends_on:
      - redpanda

volumes:
  redpanda_data:

