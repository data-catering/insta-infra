services:
  activemq:
    container_name: activemq
    environment:
      - "ARTEMIS_USER=${ARTEMIS_USER:-artemis}"
      - "ARTEMIS_PASSWORD=${ARTEMIS_PASSWORD:-artemis}"
    healthcheck:
      interval: 15s
      retries: 3
      test: [CMD-SHELL, "curl -k -f http://localhost:8161/admin"]
      timeout: 5s
    image: "apache/activemq-artemis:${ACTIVEMQ_VERSION:-2.40.0}"
    ports:
      - "61616:61616"
      - "8161:8161"
  airflow:
    command: standalone
    container_name: airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW_UID=50000
      - "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres/airflow"
      - AIRFLOW__CORE__FERNET_KEY=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=true
      - "AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 30s
      test: [CMD, curl, --fail, "http://localhost:8080/health"]
      timeout: 10s
    image: "apache/airflow:${AIRFLOW_VERSION:-3.0.0}"
    ports:
      - "8081:8080"
    restart: always
    user: "50000:0"
    volumes:
      - "./data/airflow/dags:/opt/airflow/dags"
  airflow-init:
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources=false
        if (( mem_available < 4000 )) ; then
          echo
          echo -e \033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m
          echo At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))
          echo
          warning_resources=true
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e \033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m
          echo At least 2 CPUs recommended. You have $${cpus_available}
          echo
          warning_resources=true
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e \033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m
          echo At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))
          echo
          warning_resources=true
        fi
        if [[ $${warning_resources} == true ]]; then
          echo
          echo -e \033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m
          echo Please follow the instructions to increase amount of resources available:
          echo    https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R 50000:0 /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: /bin/bash
    # yamllint enable rule:line-length
    environment:
      - AIRFLOW_UID=50000
      - "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres/airflow"
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - "_AIRFLOW_WWW_USER_USERNAME=${AIRFLOW_USER:-airflow}"
      - "_AIRFLOW_WWW_USER_PASSWORD=${AIRFLOW_PASSWORD:-airflow}"
      - _PIP_ADDITIONAL_REQUIREMENTS=
    image: "apache/airflow:${AIRFLOW_VERSION:-3.0.0}"
    user: "0:0"
  amundsen:
    command: gunicorn -w 2 --bind :5000 amundsen_application.wsgi
    container_name: amundsen
    depends_on:
      - amundsen-metadata
      - amundsen-search
    environment:
      - SEARCHSERVICE_BASE=http://amundsen-search:5000
      - METADATASERVICE_BASE=http://amundsen-metadata:5000
      - FRONTEND_SVC_CONFIG_MODULE_CLASS=amundsen_application.config.TestConfig
    image: "amundsendev/amundsen-frontend:${AMUNDSEN_FRONTEND_VERSION:-4.3.0}"
    ports:
      - "5003:5000"
  amundsen-metadata:
    command: gunicorn -w 2 --bind :5000 metadata_service.metadata_wsgi
    container_name: amundsen-metadata
    depends_on:
      - amundsen-neo4j
    environment:
      - PROXY_HOST=bolt://amundsen-neo4j
      - PROXY_ENCRYPTED=True
      - PROXY_VALIDATE_SSL=False
    image: "amundsendev/amundsen-metadata:${AMUNDSEN_METADATA_VERSION:-3.13.0}"
    ports:
      - "5002:5000"
  amundsen-neo4j:
    container_name: amundsen-neo4j
    environment:
      - NEO4J_AUTH=none
    image: "datacatering/neo4j:3.5.35"
    ports:
      - "7474:7474"
      - "7687:7687"
    ulimits:
      nofile:
        hard: 40000
        soft: 40000
    volumes:
      - "./data/amundsen-neo4j/conf:/var/lib/neo4j/conf"
  amundsen-search:
    command: gunicorn -w 2 --bind :5000 search_service.search_wsgi
    container_name: amundsen-search
    depends_on:
      - elasticsearch
    environment:
      - PROXY_ENDPOINT=elasticsearch
    image: "amundsendev/amundsen-search:${AMUNDSEN_SEARCH_VERSION:-4.2.0}"
    ports:
      - "5001:5000"
  blazer:
    command: sh -c "rails db:migrate && puma -C /app/config/puma.rb"
    container_name: blazer
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - DATABASE_URL=postgres://postgres:postgres@postgres:5432/blazer
      - DATABASE_CUSTOMER_URL=postgres://postgres:postgres@postgres:5432/customer
    image: "ankane/blazer:${BLAZER_VERSION:-v3.3.0}"
    ports:
      - "8080:8080"
    volumes:
      - "./data/blazer/blazer.yml:/app/config/blazer.yml"
  cassandra:
    command: [-c, /tmp/scripts/init.sh]
    container_name: cassandra-data
    depends_on:
      cassandra-server:
        condition: service_healthy
    entrypoint: /bin/bash
    environment:
      - DS_LICENSE=accept
    image: "datacatering/dse-server:${CASSANDRA_VERSION:-6.8.48}"
    volumes:
      - "./data/cassandra/init.sh:/tmp/scripts/init.sh"
      - "${CASSANDRA_DATA:-./data/cassandra/data}:/tmp/data"
  cassandra-server:
    cap_add:
      - IPC_LOCK
    container_name: cassandra
    environment:
      - DS_LICENSE=accept
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD-SHELL, "[ $$(nodetool statusgossip) = running ]"]
      timeout: 10s
    image: "datacatering/dse-server:${CASSANDRA_VERSION:-6.8.48}"
    ports:
      - "9042:9042"
    ulimits:
      memlock: -1
  clickhouse:
    command: [/bin/bash, -c, /tmp/scripts/init.sh]
    container_name: clickhouse-data
    depends_on:
      clickhouse-server:
        condition: service_healthy
    hostname: clickhouse
    image: "clickhouse/clickhouse-server:${CLICKHOUSE_VERSION:-25.4.1}"
    user: "101:101"
    volumes:
      - "./data/clickhouse/init.sh:/tmp/scripts/init.sh"
      - "${CLICKHOUSE_DATA:-./data/clickhouse/data}:/tmp/data"
  clickhouse-server:
    container_name: clickhouse
    depends_on:
      postgres:
        condition: service_completed_successfully
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1"
      timeout: 5s
    hostname: clickhouse
    image: "clickhouse/clickhouse-server:${CLICKHOUSE_VERSION:-25.4.1}"
    ports:
      - "8123:8123"
      - "9000:9000"
    user: "101:101"
  cockroachdb:
    command: [bash, -c, /tmp/scripts/init.sh]
    container_name: cockroachdb-data
    depends_on:
      cockroachdb-server:
        condition: service_healthy
    image: "cockroachdb/cockroach:${COCKROACHDB_VERSION:-v25.1.4}"
    volumes:
      - "./data/cockroachdb/init.sh:/tmp/scripts/init.sh"
      - "${COCKROACHDB_DATA:-./data/cockroachdb/data}:/tmp/data"
  cockroachdb-server:
    command: [start-single-node, --insecure]
    container_name: cockroachdb
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD-SHELL, "curl --fail http://localhost:8080/ || exit 1"]
      timeout: 5s
    image: "cockroachdb/cockroach:${COCKROACHDB_VERSION:-v25.1.4}"
    ports:
      - "26257:26257"
      - "8080:8080"
  confluent-schema-registry:
    container_name: confluent-schema-registry
    depends_on:
      kafka-server:
        condition: service_healthy
    env_file: ./data/confluent-schema-registry/env/docker.env
    healthcheck:
      interval: 10s
      retries: 5
      test: nc -z confluent-schema-registry 8081
      timeout: 5s
    hostname: confluent-schema-registry
    image: confluentinc/cp-schema-registry:${CONFLUENT_SCHEMA_REGISTRY_VERSION:-7.4.0}
    ports:
      - "8081:8081"
  dagster:
    container_name: dagster
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: [dagster-webserver, -h, 0.0.0.0, -p, "3000", -w, /opt/dagster/app/workspace.yaml]
    environment:
      - DAGSTER_POSTGRES_HOST=postgres
      - "DAGSTER_POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "DAGSTER_POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
      - DAGSTER_POSTGRES_DB=dagster
      - DAGSTER_HOME=/opt/dagster/dagster_home/
    image: "dagster/dagster-k8s:${DAGSTER_VERSION:-1.10.11}"
    ports:
      - "3000:3000"
    volumes:
      - "./data/dagster:/opt/dagster/app/"
  data-caterer:
    container_name: data-caterer
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - DEPLOY_MODE=standalone
    image: "datacatering/data-caterer:${DATA_CATERER_VERSION:-0.15.4}"
    ports:
      - "9898:9898"
    volumes:
      - "./data/data-caterer/connection:/opt/DataCaterer/connection"
      - "./data/data-caterer/plan:/opt/DataCaterer/plan"
  datahub:
    container_name: datahub
    depends_on:
      datahub-gms:
        condition: service_healthy
    env_file: ./data/datahub-frontend/env/docker.env
    environment:
      - ELASTIC_CLIENT_USERNAME=elastic
      - "ELASTIC_CLIENT_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    hostname: datahub
    image: acryldata/datahub-frontend-react:${DATAHUB_VERSION:-head}
    ports:
      - "9002:9002"
  datahub-actions:
    container_name: datahub-actions
    depends_on:
      datahub-gms:
        condition: service_healthy
    env_file: ./data/datahub-actions/env/docker.env
    environment:
      - ACTIONS_EXTRA_PACKAGES=${ACTIONS_EXTRA_PACKAGES:-}
      - ACTIONS_CONFIG=${ACTIONS_CONFIG:-}
    hostname: actions
    image: acryldata/datahub-actions:${DATAHUB_VERSION:-head}
  datahub-gms:
    container_name: datahub-gms
    depends_on:
      datahub-upgrade:
        condition: service_completed_successfully
    env_file: ./data/datahub-upgrade/env/docker-without-neo4j.env
    environment:
      - KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR=${KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR:-true}
      - EBEAN_DATASOURCE_USERNAME=root
      - "EBEAN_DATASOURCE_PASSWORD=${MYSQL_PASSWORD:-root}"
      - ELASTICSEARCH_USERNAME=elastic
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    healthcheck:
      interval: 1s
      retries: 3
      start_period: 90s
      test: "curl -sS --fail http://datahub-gms:8080/health"
      timeout: 5s
    hostname: datahub-gms
    image: acryldata/datahub-gms:${DATAHUB_VERSION:-head}
    ports:
      - "8080:8080"
  datahub-kafka-setup:
    container_name: datahub-kafka-setup
    depends_on:
      confluent-schema-registry:
        condition: service_healthy
      kafka-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /tmp/scripts/init.sh]
    environment:
      - "KAFKA_TOPICS=${KAFKA_TOPICS:-MetadataAuditEvent_v4,MetadataChangeEvent_v4,FailedMetadataChangeEvent_v4,MetadataChangeLog_Versioned_v1,MetadataChangeLog_Timeseries_v1,MetadataChangeProposal_v1,FailedMetadataChangeProposal_v1,PlatformEvent_v1,DataHubUpgradeHistory_v1}"
    image: "confluentinc/confluent-local:${KAFKA_VERSION:-7.9.0}"
    volumes:
      - "./data/kafka/init.sh:/tmp/scripts/init.sh"
  datahub-upgrade:
    command:
      - -u
      - SystemUpdate
    container_name: datahub-upgrade
    depends_on:
      datahub-kafka-setup:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_completed_successfully
      mysql:
        condition: service_completed_successfully
      neo4j:
        condition: service_healthy
    env_file: ./data/datahub-upgrade/env/docker-without-neo4j.env
    environment:
      - EBEAN_DATASOURCE_USERNAME=root
      - "EBEAN_DATASOURCE_PASSWORD=${MYSQL_PASSWORD:-root}"
      - ELASTICSEARCH_USERNAME=elastic
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    hostname: datahub-upgrade
    image: acryldata/datahub-upgrade:${DATAHUB_VERSION:-head}
    labels:
      datahub_setup_job: true
  debezium:
    container_name: debezium
    depends_on:
      debezium-connect:
        condition: service_healthy
    environment:
      - "KAFKA_CONNECT_URIS=http://debezium-connect:8083"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8080"]
      timeout: 10s
    image: "debezium/debezium-ui:${DEBEZIUM_VERSION:-2.1.2.Final}"
    ports:
      - "8080:8080"
  debezium-connect:
    container_name: debezium-connect
    depends_on:
      - kafka
    environment:
      - "BOOTSTRAP_SERVERS=kafka:29092"
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_connect_statuses
      - KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_REST_ADVERTISED_HOST_NAME=debezium-connect
      - CONNECT_REST_PORT=8083
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8083"]
      timeout: 10s
    image: "debezium/connect:${DEBEZIUM_CONNECT_VERSION:-2.6.2.Final}"
    ports:
      - "8083:8083"
  doris:
    container_name: doris
    depends_on:
      postgres:
        condition: service_completed_successfully
    image: "apache/doris:${DORIS_VERSION:-doris-all-in-one-2.1.0}"
    ports:
      - "8030:8030"
      - "8040:8040"
      - "9030:9030"
  druid:
    command: [router]
    container_name: druid
    depends_on:
      druid-broker:
        condition: service_healthy
      druid-coordinator:
        condition: service_healthy
      druid-historical:
        condition: service_healthy
      druid-middlemanager:
        condition: service_healthy
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8888/status/health || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.1}"
    ports:
      - "8888:8888"
  druid-broker:
    command: [broker]
    container_name: druid-broker
    depends_on:
      druid-coordinator:
        condition: service_healthy
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8082/druid/broker/v1/loadstatus || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.1}"
    ports:
      - "8082:8082"
  druid-coordinator:
    command: [coordinator]
    container_name: druid-coordinator
    depends_on:
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8081/status/health || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.1}"
    ports:
      - "8081:8081"
  druid-historical:
    command: [historical]
    container_name: druid-historical
    depends_on:
      druid-coordinator:
        condition: service_healthy
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8083/druid/historical/v1/readiness || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.1}"
    ports:
      - "8083:8083"
  druid-middlemanager:
    command: [middleManager]
    container_name: druid-middlemanager
    depends_on:
      druid-coordinator:
        condition: service_healthy
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8091/status/health || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-32.0.1}"
    ports:
      - "8091:8091"
      - "8100-8105:8100-8105"
  duckdb:
    container_name: duckdb
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: [tail, -F, anything]
    image: "datacatering/duckdb:${DUCKDB_VERSION:-v1.2.2}"
    volumes:
      - "./data/duckdb:/opt/data"
  elasticsearch:
    container_name: elasticsearch-data
    depends_on:
      elasticsearch-server:
        condition: service_healthy
    entrypoint: /tmp/entrypoint.sh
    environment:
      - "ELASTIC_PASSWORD=${ELASTIC_PASSWORD:-elasticsearch}"
      - "LOGSTASH_INTERNAL_PASSWORD=${LOGSTASH_INTERNAL_PASSWORD:-password}"
      - "KIBANA_SYSTEM_PASSWORD=${KIBANA_SYSTEM_PASSWORD:-password}"
      - "METRICBEAT_INTERNAL_PASSWORD=${METRICBEAT_INTERNAL_PASSWORD:-}"
      - "FILEBEAT_INTERNAL_PASSWORD=${FILEBEAT_INTERNAL_PASSWORD:-}"
      - "HEARTBEAT_INTERNAL_PASSWORD=${HEARTBEAT_INTERNAL_PASSWORD:-}"
      - "MONITORING_INTERNAL_PASSWORD=${MONITORING_INTERNAL_PASSWORD:-}"
      - "BEATS_SYSTEM_PASSWORD=${BEATS_SYSTEM_PASSWORD:-}"
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_VERSION:-9.0.0}"
    volumes:
      - "./data/elasticsearch/data/entrypoint.sh:/tmp/entrypoint.sh"
      - "./data/elasticsearch/data/lib.sh:/tmp/lib.sh"
      - "./data/elasticsearch/data/roles:/tmp/roles"
  elasticsearch-server:
    container_name: elasticsearch
    environment:
      - node.name=elasticsearch
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - "ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
      - XPACK_SECURITY_ENABLED=true
      - discovery.type=single-node
    healthcheck:
      interval: 10s
      retries: 5
      test: "curl -sS --fail http://elasticsearch:9200/_cluster/health?wait_for_status=yellow&timeout=0s"
      timeout: 5s
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_VERSION:-9.0.0}"
    ports:
      - "9200:9200"
      - "9300:9300"
    restart: unless-stopped
    volumes:
      - "./data/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro,Z"
  evidence:
    container_name: evidence
    image: "evidencedev/devenv:${EVIDENCE_VERSION:-latest}"
    init: true
    ports:
      - "3000:3000"
    volumes:
      - "./data/evidence/persist:/evidence-workspace"
  feast:
    command: [feast, serve, -h, 0.0.0.0]
    container_name: feast
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD, feast, status]
      timeout: 10s
    image: "feastdev/feature-server:${FEAST_VERSION:-0.46.0}"
    ports:
      - "${FEAST_PORT:-6566}:6566"
  flight-sql:
    command: [tail, -f, /dev/null]
    container_name: flight-sql
    depends_on:
      - duckdb
      - sqlite
    environment:
      - TLS_ENABLED=1
      - "FLIGHT_PASSWORD=${FLIGHT_SQL_PASSWORD:-flight_password}"
      - PRINT_QUERIES=1
    image: "voltrondata/flight-sql:${FLIGHT_SQL_VERSION:-v1.4.1}"
    ports:
      - "31337:31337"
  flink:
    command: taskmanager
    container_name: flink
    depends_on:
      - flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - "FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager"
    expose:
      - 6121
      - 6122
    image: "flink:${FLINK_VERSION:-2.0.0-scala_2.12-java17}"
    links: []
  flink-jobmanager:
    command: jobmanager
    container_name: flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - "FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager"
    expose:
      - 6123
    image: "flink:${FLINK_VERSION:-2.0.0-scala_2.12-java17}"
    ports:
      - "8081:8081"
  fluentd:
    container_name: fluentd
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
    image: "datacatering/fluentd-elasticsearch:${FLUENTD_VERSION:-v1.17.0-debian-1.0}"
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    volumes:
      - "./data/fluentd/etc/fluent.conf:/fluentd/etc/fluent.conf"
  grafana:
    container_name: grafana
    environment:
      - "GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}"
      - "GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}"
      - GF_SECURITY_ANGULAR_SUPPORT_ENABLED=true
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_SECURITY_COOKIE_SAMESITE=none
      - GF_SECURITY_COOKIE_SECURE=true
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      timeout: 5s
    image: "grafana/grafana:${GRAFANA_VERSION:-11.6.0}"
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - "./data/grafana/dashboards:/etc/grafana/dashboards"
      - "./data/grafana/provisioning:/etc/grafana/provisioning"
  httpbin:
    container_name: http
    environment:
      - "GUNICORN_CMD_ARGS=--capture-output --error-logfile - --access-logfile - --access-logformat '%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s}'"
    image: "kennethreitz/httpbin:${HTTPBIN_VERSION:-latest}"
    ports:
      - "8080:80"
  httpd:
    container_name: httpd
    depends_on:
      - fluentd
    image: "httpd:${HTTPD_VERSION:-2.4.63}"
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        fluentd-async-connect: "true"
        tag: httpd.access
    ports:
      - "8080:80"
  influxdb:
    container_name: influxdb
    environment:
      - "DOCKER_INFLUXDB_INIT_USERNAME=${INFLUXDB_USER:-admin}"
      - "DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUXDB_PASSWORD:-admin}"
      - DOCKER_INFLUXDB_INIT_ORG=myorg
      - DOCKER_INFLUXDB_INIT_BUCKET=mybucket
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8086/health"]
      timeout: 5s
    image: "influxdb:${INFLUXDB_VERSION:-2.7}"
    ports:
      - "${INFLUXDB_PORT:-8086}:8086"
  jaeger:
    container_name: jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, wget, --no-verbose, --tries=1, --spider, "http://localhost:16686"]
      timeout: 5s
    image: "jaegertracing/all-in-one:${JAEGER_VERSION:-1.60}"
    ports:
      - "${JAEGER_PORT:-16686}:16686"
      - "14250:14250"
      - "14268:14268"
      - "6831:6831/udp"
  jupyter:
    command: [jupyter, notebook, --no-browser, "--NotebookApp.token=''", "--NotebookApp.password=''"]
    container_name: jupyter
    image: "quay.io/jupyter/minimal-notebook:${JUPYTER_VERSION:-2024-07-02}"
    ports:
      - "8888:8888"
  kafka:
    container_name: kafka-data
    depends_on:
      kafka-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /tmp/scripts/init.sh]
    environment:
      - "KAFKA_TOPICS=${KAFKA_TOPICS:-accounts,transactions}"
    image: "confluentinc/confluent-local:${KAFKA_VERSION:-7.9.0}"
    volumes:
      - "./data/kafka/init.sh:/tmp/scripts/init.sh"
  kafka-server:
    container_name: kafka
    environment:
      - "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - "KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092"
      - "KAFKA_LISTENERS=PLAINTEXT://kafka:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092"
    expose:
      - 29092
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD-SHELL, /bin/sh, -c, kafka-topics, --bootstrap-server, "kafka:29092", --list]
      timeout: 5s
    image: "confluentinc/confluent-local:${KAFKA_VERSION:-7.9.0}"
    ports:
      - "9092:9092"
  keycloak:
    command: [start-dev, --import-realm]
    container_name: keycloak
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - KC_DB=postgres
      - "KC_DB_USERNAME=${POSTGRES_USER:-postgres}"
      - "KC_DB_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
      - "KC_DB_URL=jdbc:postgresql://postgres:5432/keycloak"
      - KC_REALM_NAME=myrealm
      - "KEYCLOAK_ADMIN=${KEYCLOAK_USER:-admin}"
      - "KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_PASSWORD:-admin}"
    image: "quay.io/keycloak/keycloak:${KEYCLOACK_VERSION:-26.2.0}"
    ports:
      - "8082:8080"
    restart: unless-stopped
    volumes:
      - "./data/keycloak/realm.json:/opt/keycloak/data/import/realm.json:ro"
  kibana:
    container_name: kibana
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - "KIBANA_SYSTEM_PASSWORD=${KIBANA_SYSTEM_PASSWORD:-password}"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:5601/api/status"]
      timeout: 5s
    image: "docker.elastic.co/kibana/kibana:${KIBANA_VERSION:-9.0.0}"
    ports:
      - "5601:5601"
    restart: always
    volumes:
      - "./data/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml"
  kong:
    container_name: kong
    depends_on:
      kong-data:
        condition: service_completed_successfully
      postgres:
        condition: service_completed_successfully
    environment:
      - KONG_DATABASE=postgres
      - KONG_PG_HOST=postgres
      - KONG_PG_USER=postgres
      - KONG_PG_PASSWORD=postgres
      - KONG_PROXY_ACCESS_LOG=/dev/stdout
      - KONG_ADMIN_ACCESS_LOG=/dev/stdout
      - KONG_PROXY_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_LISTEN=0.0.0.0:8001,0.0.0.0:8444 ssl
      - KONG_PLUGINS=rate-limiting
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD-SHELL, kong health]
      timeout: 10s
    image: "kong:${KONG_VERSION:-3.9.0}"
    ports:
      - 8001:8001
      - 8000:8000
    restart: unless-stopped
  kong-data:
    command:
      - kong
      - migrations
      - bootstrap
    container_name: kong-data
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - KONG_DATABASE=postgres
      - KONG_PG_HOST=postgres
      - KONG_PG_USER=postgres
      - KONG_PG_PASSWORD=postgres
    image: "kong:${KONG_VERSION:-3.9.0}"
  # Apache Iceberg Lakekeeper Catalog
  # https://docs.lakekeeper.io/docs/nightly/configuration/
  lakekeeper:
    command: [serve]
    depends_on:
      lakekeeper-data:
        condition: service_healthy
      lakekeeper-mc:
        condition: service_completed_successfully
      lakekeeper-migrate:
        condition: service_completed_successfully
      lakekeeper-minio:
        condition: service_healthy
    environment:
      - LAKEKEEPER__PG_ENCRYPTION_KEY=This-is-NOT-Secure!
      - LAKEKEEPER__PG_DATABASE_URL_READ=postgresql://postgres:postgres@lakekeeper-data:5432/postgres
      - LAKEKEEPER__PG_DATABASE_URL_WRITE=postgresql://postgres:postgres@lakekeeper-data:5432/postgres
      - LAKEKEEPER__AUTHZ_BACKEND=openfga
      - LAKEKEEPER__OPENFGA__ENDPOINT=http://lakekeeper-openfga:8081
      - LAKEKEEPER__OPENID_PROVIDER_URI=http://lakekeeper-keycloak:8080/realms/iceberg
      - LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=http://localhost:30080/realms/iceberg
      - LAKEKEEPER__OPENID_AUDIENCE=lakekeeper
      - LAKEKEEPER__UI__OPENID_CLIENT_ID=lakekeeper
      - LAKEKEEPER__UI__OPENID_PROVIDER_URI=http://localhost:30080/realms/iceberg
      - RUST_LOG=info
    healthcheck:
      interval: 1s
      retries: 3
      start_period: 3s
      test: [CMD, /home/nonroot/iceberg-catalog, healthcheck]
      timeout: 10s
    image: quay.io/lakekeeper/catalog:${LAKEKEEPER_IMAGE:-latest-main}
    ports:
      - "8183:8181"
    pull_policy: if_not_present
  # Lakekeeper Iceberg catalog backing store
  lakekeeper-data:
    environment:
      - "POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
      - PGDATA=/data/postgres
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, pg_isready -U postgres -p 5432 -d postgres]
      timeout: 5s
    image: "postgres:${POSTGRES_VERSION:-17.4}"
    ports:
      - "5432:5432"
  # Lakekeeper Jupyter Notebook with built-in Spark and demo notebooks
  # https://docs.lakekeeper.io/docs/nightly/bootstrap/
  lakekeeper-jupyter:
    command: start-notebook.sh --NotebookApp.token=''
    depends_on:
      lakekeeper:
        condition: service_healthy
      lakekeeper-trino:
        condition: service_healthy
      lakekeeper-trino-proxy:
        condition: service_healthy
    image: quay.io/jupyter/pyspark-notebook:${LAKEKEEPER_JUPYTER_VERSION:-2024-10-14}
    ports:
      - "8889:8888"
    volumes:
      - ./data/lakekeeper/access-control-advanced/notebooks:/home/jovyan/examples/
  # Lakekeeper IdP with Keycloak
  # https://docs.lakekeeper.io/docs/nightly/authentication/#keycloak
  lakekeeper-keycloak:
    command: [start-dev, --metrics-enabled=true, --health-enabled=true, --import-realm, --verbose, --log-level=INFO, --features=token-exchange]
    environment:
      - KC_BOOTSTRAP_ADMIN_USERNAME=admin
      - KC_BOOTSTRAP_ADMIN_PASSWORD=admin
    healthcheck:
      interval: 2s
      retries: 2
      start_period: 60s
      test: [CMD-SHELL, 'exec 3<>/dev/tcp/127.0.0.1/8080;echo -e "GET /realms/master/.well-known/openid-configuration HTTP/1.1\r\nhost: 127.0.0.1:8080\r\nConnection: close\r\n\r\n" >&3;grep "jwks_uri"  <&3']
      timeout: 10s
    image: quay.io/keycloak/keycloak:${LAKEKEEPER_KEYCLOAK_VERSION:-26.0.7}
    ports:
      - "30080:8080"
    volumes:
      - ./data/lakekeeper/access-control-advanced/keycloak/realm.json:/opt/keycloak/data/import/realm.json
  # Lakekeeper MinIO bucket initialization
  # Creates bucket `examples`
  lakekeeper-mc:
    container_name: mc
    depends_on:
      - lakekeeper-minio
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://lakekeeper-minio:9000 minio-root-user minio-root-password) do echo '...waiting...' && sleep 5; done;
      /usr/bin/mc mb minio/examples;
      /usr/bin/mc anonymous set public minio/examples;
      " 
    environment:
      - MINIO_ROOT_USER=minio-root-user
      - MINIO_ROOT_PASSWORD=minio-root-password
    image: minio/mc
    restart: no
  # Lakekeeper database migration
  lakekeeper-migrate:
    depends_on:
      lakekeeper-data:
        condition: service_healthy
      lakekeeper-keycloak:
        condition: service_healthy
      lakekeeper-openfga:
        condition: service_healthy
    entrypoint: [/home/nonroot/iceberg-catalog, migrate]
    environment:
      - LAKEKEEPER__PG_ENCRYPTION_KEY=This-is-NOT-Secure!
      - LAKEKEEPER__PG_DATABASE_URL_READ=postgresql://postgres:postgres@lakekeeper-data:5432/postgres
      - LAKEKEEPER__PG_DATABASE_URL_WRITE=postgresql://postgres:postgres@lakekeeper-data:5432/postgres
      - LAKEKEEPER__AUTHZ_BACKEND=openfga
      - LAKEKEEPER__OPENFGA__ENDPOINT=http://lakekeeper-openfga:8081
      - RUST_LOG=info
    image: quay.io/lakekeeper/catalog:${LAKEKEEPER_IMAGE_VERSION:-latest-main}
    pull_policy: if_not_present
    restart: no
  # Lakekeeper MinIO object storage
  # https://docs.lakekeeper.io/docs/nightly/storage/#s3-compatible
  lakekeeper-minio:
    command: [server, /data, --console-address, ":9001"]
    environment:
      - MINIO_ROOT_USER=minio-root-user
      - MINIO_ROOT_PASSWORD=minio-root-password
      - MINIO_API_PORT_NUMBER=9000
      - MINIO_CONSOLE_PORT_NUMBER=9001
      - MINIO_SCHEME=http
      - MINIO_DEFAULT_BUCKETS=examples
      - MINIO_DATA_DIR=/data
    healthcheck:
      interval: 2s
      retries: 2
      start_period: 15s
      test: [CMD, mc, ready, local]
      timeout: 10s
    image: "quay.io/minio/minio:${MINIO_VERSION:-RELEASE.2025-04-08T15-41-24Z}"
    ports:
      - "9900:9000"
      - "9901:9001"
  # Lakekeeper OpenPolicy Agent for Trino
  # https://docs.lakekeeper.io/docs/nightly/opa/
  lakekeeper-opa:
    command: [run, --server, --log-format, text, --log-level, debug, --addr, "0.0.0.0:8181", /policies]
    depends_on:
      lakekeeper:
        condition: service_healthy
    environment:
      - LAKEKEEPER_URL=http://lakekeeper:8181
      - LAKEKEEPER_TOKEN_ENDPOINT=http://lakekeeper-keycloak:8080/realms/iceberg/protocol/openid-connect/token
      - LAKEKEEPER_CLIENT_ID=trino
      - LAKEKEEPER_CLIENT_SECRET=AK48QgaKsqdEpP9PomRJw7l2T7qWGHdZ
      - LAKEKEEPER_SCOPE=lakekeeper
      - TRINO_LAKEKEEPER_CATALOG_NAME=lakekeeper
      - LAKEKEEPER_LAKEKEEPER_WAREHOUSE=demo
    image: openpolicyagent/opa:${LAKEKEEPER_OPA_VERSION:-1.0.0}
    volumes:
      - ./data/lakekeeper/access-control-advanced/authz/opa-bridge/policies:/policies
  # Authorization backend for Lakekeeper
  # https://docs.lakekeeper.io/docs/nightly/authorization/
  lakekeeper-openfga:
    command: run
    depends_on:
      lakekeeper-openfga-data:
        condition: service_healthy
      lakekeeper-openfga-migrate:
        condition: service_completed_successfully
    environment:
      - OPENFGA_DATASTORE_ENGINE=postgres
      - OPENFGA_DATASTORE_URI=postgres://postgres:postgres@lakekeeper-openfga-data:5432/postgres?sslmode=disable
      - OPENFGA_DATASTORE_MAX_OPEN_CONNS=50
      - OPENFGA_PLAYGROUND_ENABLED=false
    healthcheck:
      interval: 5s
      retries: 3
      test:
        - CMD
        - /usr/local/bin/grpc_health_probe
        - "-addr=lakekeeper-openfga:8081"
        
      timeout: 30s
    image: openfga/openfga:${LAKEKEEPER_OPENFGA_VERSION:-v1.8}
  # Lakekeeper OpenFGA database
  lakekeeper-openfga-data:
    environment:
      - POSTGRESQL_USERNAME=postgres
      - POSTGRESQL_PASSWORD=postgres
      - POSTGRESQL_DATABASE=postgres
    healthcheck:
      interval: 2s
      retries: 2
      start_period: 10s
      test: [CMD-SHELL, pg_isready -U postgres -p 5432 -d postgres]
      timeout: 10s
    image: bitnami/postgresql:17.4.0
  # Lakekeeper OpenFGA database migration
  lakekeeper-openfga-migrate:
    command: migrate
    depends_on:
      lakekeeper-openfga-data:
        condition: service_healthy
    environment:
      - OPENFGA_DATASTORE_ENGINE=postgres
      - OPENFGA_DATASTORE_URI=postgres://postgres:postgres@lakekeeper-openfga-data:5432/postgres?sslmode=disable
    image: openfga/openfga:${LAKEKEEPER_OPENFGA_VERSION:-v1.8}
  # Lakekeeper-connected Trino with OAuth2 Authentication and OPA access control enabled
  # https://docs.lakekeeper.io/docs/nightly/engines/#trino
  # https://trino.io/docs/current/security/opa-access-control.html
  lakekeeper-trino:
    depends_on:
      lakekeeper-opa:
        condition: service_started
    environment:
      - OIDC_ISSUER=http://localhost:30080/realms/iceberg
      - DISCOVERY_URI=http://lakekeeper-trino:8080
      - OAUTH2_AUTH_URL=http://localhost:30080/realms/iceberg/protocol/openid-connect/auth
      - OAUTH2_JWKS_URL=http://lakekeeper-keycloak:8080/realms/iceberg/protocol/openid-connect/certs
      - OAUTH2_TOKEN_URL=http://lakekeeper-keycloak:8080/realms/iceberg/protocol/openid-connect/token
      - OAUTH2_CLIENT_ID=trino
      - OAUTH2_CLIENT_SECRET=AK48QgaKsqdEpP9PomRJw7l2T7qWGHdZ
      - OAUTH2_SCOPES=trino
      - INTERNAL_COMMUNICATION_SHARED_SECRET="MxeWYqiiD2vSFHm86T/jRr49PUYT3xRbVTlLwUTj7lSqt/wXqDb+d6r70yJz2MmmwOB8kV8tKnxfChyJFZk1FIhtgwlJZu18kExdZRnYQttKzeEGjsDyUMiX43cMWaHcqVVReuazfmg1EWwuwRhi40VawZKTiXh73k/02a/qVJDjWtQWcpTCj1+1OMity7zvrmLGO6XPpSqzY8YsZGuZKUDAbtjw2fifPTYThzSKDSiVPEeZQ0n7mpHylmvEyFxrZWtUWHRXIEJi2XbgBXsAOHyjwxbA2Uq0LIMXhaKHyQDoK4NGyGxV+is1BkCykBfq8IqGGty/HaOgBeEUWNEUugvqc10TyHTbSiGmbilY4bZJv+ol3wn1mtyMtkWF7v8tFCh8y0dyC7CPcXbZLg0FRDfe2ewjZgbt3/hxhmHHNrBOha/nUeUT44UqGPzf+xo8fGvcpo+0hxIxDPSSdEDMUrArfP5iebv2Esdp3+7WPFzvtpTxZIKz3FZdazd+iRaJ1pGDTFn8cTy5Owx1P8bsofFXegfoCqX2ShAXHVGNNkWo/Z66lofzQnRk+lO1b2cDybPj8LkukfMC0EgE7SdS7GXAfp3Yary82CPc38deFVffqLSFqGcVNPmiMTgJyauKZS/XjgtObK2ZvjGAwC7Mc9RsWr+xc75t768wvt5UIXo="
      - OPA_POLICY_URI=http://lakekeeper-opa:8181/v1/data/trino/allow
      - OPA_POLICY_BATCHED_URI=http://lakekeeper-opa:8181/v1/data/trino/batch
    healthcheck:
      interval: 2s
      retries: 2
      start_period: 10s
      test: [CMD, curl, -I, "http://localhost:8080/v1/status"]
      timeout: 10s
    image: trinodb/trino:${LAKEKEEPER_TRINO_VERSION:-467}
    ports:
      - "8880:8080"
    volumes:
      - ./data/lakekeeper/access-control-advanced/trino/access-control.properties:/etc/trino/access-control.properties
      - ./data/lakekeeper/access-control-advanced/trino/config.properties:/etc/trino/config.properties
      - ./data/lakekeeper/access-control-advanced/trino/log.properties:/etc/trino/log.properties
  # Lakekeeper Trino Proxy emulates a reverse proxy that secures the connection to the server.
  # This is required as trino requires HTTPS in OAUTH2 settings.
  lakekeeper-trino-proxy:
    depends_on:
      lakekeeper-trino:
        condition: service_healthy
    healthcheck:
      interval: 1s
      retries: 3
      start_period: 3s    
      test: [CMD, service, nginx, status]
      timeout: 10s
    image: nginx
    ports:
      - 443:443
      - 38191:38191
    volumes:
      - ./data/lakekeeper/access-control-advanced/nginx.conf:/etc/nginx/nginx.conf
      - ./data/lakekeeper/access-control-advanced/certs:/etc/nginx/certs
  logstash:
    container_name: logstash
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
    environment:
      LOGSTASH_INTERNAL_PASSWORD: "${LOGSTASH_INTERNAL_PASSWORD:-password}"
      LS_JAVA_OPTS: -Xms256m -Xmx256m
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:9600"]
      timeout: 5s
    image: "docker.elastic.co/logstash/logstash:${LOGSTASH_VERSION:-9.0.0}"
    ports:
      - "5044:5044"
      - "50000:50000/tcp"
      - "50000:50000/udp"
      - "9600:9600"
    restart: unless-stopped
    volumes:
      - "./data/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml"
      - "./data/logstash/pipeline:/usr/share/logstash/pipeline"
  loki:
    command: -config.file=/etc/loki/local-config.yaml
    container_name: loki
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      timeout: 5s
    image: "grafana/loki:${LOKI_VERSION:-3.5.0}"
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - "./data/loki/config.yaml:/etc/loki/local-config.yaml"
  maestro:
    container_name: maestro
    depends_on:
      cockroachdb:
        condition: service_completed_successfully
    environment:
      - CONDUCTOR_CONFIGS_JDBCURL=jdbc:postgresql://cockroachdb:26257/maestro
      - CONDUCTOR_CONFIGS_JDBCUSERNAME=root
    image: "datacatering/maestro:${MAESTRO_VERSION:-0.1.0}"
    ports:
      - "8081:8080"
  mage-ai:
    command: mage start your_first_project
    container_name: mage-ai
    environment:
      - USER_CODE_PATH=/home/src/your_first_project
    image: "mageai/mageai:${MAGE_AI_VERSION:-0.9.76}"
    ports:
      - "6789:6789"
    restart: on-failure
  mariadb:
    container_name: mariadb
    environment:
      - "MARIADB_USER=${MARIADB_USER:-user}"
      - "MARIADB_PASSWORD=${MARIADB_PASSWORD:-password}"
      - MARIADB_ROOT_PASSWORD=root
      - MARIADB_DATABASE=customer
    image: "mariadb:${MARIADB_VERSION:-11.7.2}"
    ports:
      - "3306:3306"
    restart: always
  marquez:
    container_name: marquez-web
    depends_on:
      - marquez-data
    environment:
      - MARQUEZ_HOST=host.docker.internal
      - MARQUEZ_PORT=5002
    image: "marquezproject/marquez-web:${MARQUEZ_VERSION:-0.51.1}"
    ports:
      - "3001:3000"
  marquez-data:
    command: [-c, /tmp/scripts/init.sh]
    container_name: marquez-data
    depends_on:
      marquez-server:
        condition: service_healthy
    entrypoint: /bin/bash
    environment:
      - "MARQUEZ_URL=http://marquez:5000"
    image: "marquezproject/marquez:${MARQUEZ_VERSION:-0.51.1}"
    volumes:
      - "./data/marquez/init.sh:/tmp/scripts/init.sh"
      - "${MARQUEZ_DATA:-./data/marquez/data}:/tmp/data"
  marquez-server:
    container_name: marquez
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - MARQUEZ_CONFIG=/opt/app/marquez.yaml
      - MARQUEZ_PORT=5000
      - MARQUEZ_ADMIN_PORT=5001
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=marquez
      - "POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:5001/healthcheck"]
      timeout: 5s
    image: "marquezproject/marquez:${MARQUEZ_VERSION:-0.51.1}"
    ports:
      - "5002:5000"
      - "5001:5001"
    volumes:
      - "./data/marquez/conf:/opt/app"
  metabase:
    container_name: metabase
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      MB_DB_DBNAME: metabase
      MB_DB_HOST: postgres
      MB_DB_PASS: postgres
      MB_DB_PORT: 5432
      MB_DB_TYPE: postgres
      MB_DB_USER: postgres
    healthcheck:
      interval: 15s
      retries: 5
      test: [CMD-SHELL, curl, --fail, -I, http://localhost:3000/api/health || exit 1]
      timeout: 5s
    image: "metabase/metabase:${METABASE_VERSION:-v0.54.4}"
    ports:
      - "3000:3000"
    volumes:
      - /dev/urandom:/dev/random:ro
  milvus:
    command: [standalone]
    container_name: milvus
    environment:
      - ETCD_ENDPOINTS=localhost:2379
      - MINIO_ADDRESS=localhost
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD, curl, -f, "http://localhost:9091/healthz"]
      timeout: 10s
    image: "milvusdb/milvus:${MILVUS_VERSION:-v2.5.10}"
    ports:
      - "${MILVUS_PORT:-19530}:19530"
      - "9091:9091"
  minio:
    command: [server, /data, --console-address, ":9001"]
    container_name: minio
    environment:
      - "MINIO_ROOT_USER=${MINIO_USER:-minioadmin}"
      - "MINIO_ROOT_PASSWORD=${MINIO_PASSWORD:-minioadmin}"
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, mc, ready, local]
      timeout: 5s
    image: "quay.io/minio/minio:${MINIO_VERSION:-RELEASE.2024-06-04T19-20-08Z}"
    ports:
      - "9000:9000"
      - "9001:9001"
  mlflow:
    command: mlflow server --host 0.0.0.0
    container_name: mlflow
    environment:
      - "MLFLOW_TRACKING_URI=http://localhost:5000"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:5000/health"]
      timeout: 5s
    image: "ghcr.io/mlflow/mlflow:${MLFLOW_VERSION:-v2.21.3}"
    ports:
      - "${MLFLOW_PORT:-5000}:5000"
  mongodb:
    command: [/bin/sh, -c, /opt/app/my_data.sh]
    container_name: mongodb-connect
    depends_on:
      - mongodb-server
    environment:
      - "CONN_STR=mongodb://${MONGODB_USER:-user}:${MONGODB_PASSWORD:-password}@mongodb-server"
    image: "mongodb/mongodb-community-server:${MONGODB_VERSION:-8.0.8-ubi8}"
    volumes:
      - "./data/mongodb:/opt/app"
  mongodb-server:
    container_name: mongodb
    environment:
      - "MONGO_INITDB_ROOT_USERNAME=${MONGODB_USER:-user}"
      - "MONGO_INITDB_ROOT_PASSWORD=${MONGODB_PASSWORD:-password}"
    image: "mongo:${MONGODB_VERSION:-8.0.8}"
    ports:
      - "27017:27017"
  mssql:
    container_name: mssql
    environment:
      - "SA_PASSWORD=${MSSQL_PASSWORD:-yourStrong(!)Password}"
      - ACCEPT_EULA=Y
    healthcheck:
      interval: 10s
      retries: 10
      test: [CMD-SHELL, mssql-health-check]
      timeout: 10s
    image: "mcr.microsoft.com/mssql/server:${MSSQL_VERSION:-2022-latest}"
    ports:
      - "1433:1433"
    volumes:
      - "./data/mssql/mssql-health-check:/usr/local/bin/mssql-health-check"
  mysql:
    command: [/bin/bash, -c, /tmp/scripts/init.sh]
    container_name: mysql-data
    depends_on:
      mysql-server:
        condition: service_healthy
    environment:
      - "MYSQL_PASSWORD=${MYSQL_PASSWORD:-root}"
    image: "mysql:${MYSQL_VERSION:-9.3.0}"
    volumes:
      - "./data/mysql/init.sh:/tmp/scripts/init.sh"
      - "${MYSQL_DATA:-./data/mysql/data}:/tmp/data"
  mysql-server:
    container_name: mysql
    environment:
      - "MYSQL_ROOT_PASSWORD=${MYSQL_PASSWORD:-root}"
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, mysqladmin, ping, -h, localhost, -u, root, -p$$MYSQL_ROOT_PASSWORD]
      timeout: 5s
    image: "mysql:${MYSQL_VERSION:-9.3.0}"
    ports:
      - "3306:3306"
  nats:
    command: [--http_port, "8222"]
    container_name: nats
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, /bin/sh, -c, "curl -f http://localhost:8222/healthz"]
      timeout: 5s
    image: "nats:${NATS_VERSION:-2.11.1}"
    ports:
      - "${NATS_PORT:-4222}:4222"
      - "8222:8222"
  neo4j:
    container_name: neo4j
    environment:
      - NEO4J_AUTH=none
    healthcheck:
      interval: 30s
      retries: 5
      test: [CMD-SHELL, "cypher-shell -u neo4j -p test 'RETURN 1' || exit 1"]
      timeout: 10s
    image: "neo4j:${NEO4J_VERSION:-2025.03.0}"
    ports:
      - "7474:7474"
      - "7687:7687"
  openmetadata:
    command:
      - /opt/airflow/ingestion_dependency.sh
    container_name: openmetadata-ingestion
    depends_on:
      openmetadata-server:
        condition: service_healthy
    entrypoint: /bin/bash
    env_file: ./data/openmetadata-ingestion/env/docker.env
    environment:
      - "DB_USER=${MYSQL_USER:-root}"
      - "DB_USER_PASSWORD=${MYSQL_PASSWORD:-root}"
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 30s
      test: [CMD, curl, --fail, "http://localhost:8080/health"]
      timeout: 10s
    image: "docker.getcollate.io/openmetadata/ingestion:${OPENMETADATA_VERSION:-1.7.0}"
    ports:
      - "8080:8080"
  openmetadata-server:
    container_name: openmetadata
    depends_on:
      openmetadata-setup:
        condition: service_completed_successfully
    env_file: ./data/openmetadata/env/docker.env
    environment:
      - "DB_USER=${MYSQL_USER:-root}"
      - "DB_USER_PASSWORD=${MYSQL_PASSWORD:-root}"
      - "ELASTICSEARCH_USER=${ELASTICSEARCH_USER:-elastic}"
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD, wget, -q, --spider,  "http://localhost:8586/healthcheck"]
      timeout: 5s
    image: "docker.getcollate.io/openmetadata/server:${OPENMETADATA_VERSION:-1.7.0}"
    ports:
      - "8585:8585"
      - "8586:8586"
    restart: always
  openmetadata-setup:
    command: ./bootstrap/openmetadata-ops.sh migrate
    container_name: openmetadata-setup
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
      mysql:
        condition: service_completed_successfully
    env_file: ./data/openmetadata/env/docker.env
    environment:
      - "DB_USER=${MYSQL_USER:-root}"
      - "DB_USER_PASSWORD=${MYSQL_PASSWORD:-root}"
      - "ELASTICSEARCH_USER=${ELASTICSEARCH_USER:-elastic}"
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    image: "docker.getcollate.io/openmetadata/server:${OPENMETADATA_VERSION:-1.7.0}"
  opensearch:
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - "OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_PASSWORD:-!BigData#1}"
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD, curl, --fail, "https://localhost:9200", -ku, "admin:${OPENSEARCH_PASSWORD:-!BigData#1}"]
      timeout: 5s
    image: "opensearchproject/opensearch:${OPENSEARCH_VERSION:-2.19.1}"
    ports:
      - "9600:9600"
      - "9200:9200"
  pinot:
    command: "StartServer -zkAddress zookeeper:2181"
    container_name: pinot-server
    depends_on:
      pinot-broker:
        condition: service_healthy
    environment:
      - "JAVA_OPTS=-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx16G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-server.log"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8098/health/readiness"]
      timeout: 5s
    image: "apachepinot/pinot:${PINOT_VERSION:-1.3.0}"
    ports:
      - "8098:8098"
    restart: unless-stopped
  pinot-broker:
    command: "StartBroker -zkAddress zookeeper:2181"
    container_name: pinot-broker
    depends_on:
      pinot-controller:
        condition: service_healthy
    environment:
      - "JAVA_OPTS=-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-broker.log"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8099/health"]
      timeout: 5s
    image: "apachepinot/pinot:${PINOT_VERSION:-1.3.0}"
    ports:
      - "8099:8099"
    restart: unless-stopped
  pinot-controller:
    command: "StartController -zkAddress zookeeper:2181"
    container_name: pinot
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      - "JAVA_OPTS=-Dplugins.dir=/opt/pinot/plugins -Xms1G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-controller.log"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:9000/pinot-controller/admin"]
      timeout: 5s
    image: "apachepinot/pinot:${PINOT_VERSION:-1.3.0}"
    ports:
      - "9000:9000"
    restart: unless-stopped
  polaris:
    container_name: polaris
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD, curl, "http://localhost:8182/healthcheck"]
      timeout: 10s
    image: "datacatering/polaris:${POLARIS_VERSION:-1.0.0}"
    ports:
      - "8181:8181"
      - "8182:8182"
  postgres:
    command: [/bin/bash, -c, /tmp/scripts/init.sh]
    container_name: postgres-data
    depends_on:
      postgres-server:
        condition: service_healthy
    environment:
      - "POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "PGPASSWORD=${POSTGRES_PASSWORD:-postgres}"
    image: "postgres:${POSTGRES_VERSION:-17.4}"
    volumes:
      - "./data/postgres/init.sh:/tmp/scripts/init.sh"
      - "${POSTGRES_DATA:-./data/postgres/data}:/tmp/data"
  postgres-server:
    container_name: postgres
    environment:
      - "POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
      - PGDATA=/data/postgres
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, pg_isready]
      timeout: 5s
    image: "postgres:${POSTGRES_VERSION:-17.4}"
    ports:
      - "5432:5432"
  prefect:
    container_name: prefect-data
    depends_on:
      - prefect-server
    entrypoint: [/opt/prefect/app/start_flows.sh]
    environment:
      - "PREFECT_API_URL=http://host.docker.internal:4200/api"
    image: "prefecthq/prefect:${PREFECT_VERSION:-3.3.5-python3.11}"
    volumes:
      - "./data/prefect/flows:/root/flows"
      - "./data/prefect/start_flows.sh:/opt/prefect/app/start_flows.sh"
    working_dir: /root/flows
  prefect-server:
    container_name: prefect
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: [/opt/prefect/entrypoint.sh, prefect, server, start]
    environment:
      - "PREFECT_UI_URL=http://127.0.0.1:4200/api"
      - "PREFECT_API_URL=http://127.0.0.1:4200/api"
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - "PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/prefect"
    image: "prefecthq/prefect:${PREFECT_VERSION:-3.3.5-python3.11}"
    ports:
      - "4200:4200"
    restart: always
  presto:
    container_name: presto
    depends_on:
      postgres:
        condition: service_completed_successfully
    image: "prestodb/presto:${PRESTO_VERSION:-0.292}"
    ports:
      - "8083:8080"
    volumes:
      - "./data/presto/etc:/opt/presto-server/etc"
      - "./data/presto/catalog:/opt/presto-server/etc/catalog"
  prometheus:
    command: --config.file=/etc/prometheus/prometheus.yml
    container_name: prometheus
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, wget, --no-verbose, --tries=1, --spider, "http://localhost:9090/-/healthy"]
      timeout: 5s
    image: "prom/prometheus:${PROMETHEUS_VERSION:-v3.3.0}"
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - "./data/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml"
  pulsar:
    command: [bin/pulsar, standalone]
    container_name: pulsar
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD, bin/pulsar-admin, brokers, healthcheck]
      timeout: 10s
    image: "apachepulsar/pulsar:${PULSAR_VERSION:-4.0.4}"
    ports:
      - "${PULSAR_PORT:-6650}:6650"
      - "8080:8080"
    volumes:
      - "./data/pulsar/conf:/pulsar/conf"
  qdrant:
    container_name: qdrant
    environment:
      - QDRANT_ALLOW_RECOVERY_MODE=true
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, -f, "http://localhost:6333/health"]
      timeout: 5s
    image: "qdrant/qdrant:${QDRANT_VERSION:-v1.14.0}"
    ports:
      - "${QDRANT_PORT:-6333}:6333"
      - "6334:6334"
  rabbitmq:
    container_name: rabbitmq
    environment:
      - "RABBITMQ_DEFAULT_USER=${RABBITMQ_USER:-guest}"
      - "RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD:-guest}"
    healthcheck:
      interval: 30s
      retries: 3
      test: rabbitmq-diagnostics -q ping
      timeout: 30s
    hostname: my-rabbit
    image: "rabbitmq:${RABBITMQ_VERSION:-4.1.0-management}"
    ports:
      - "5672:5672"
      - "15672:15672"
  ray:
    command: [bash, -c, ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265 --metrics-export-port=8080 --block]
    container_name: ray
    depends_on:
      grafana:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    environment:
      - HOST=0.0.0.0
      - RAY_ENABLE_PROMETHEUS_METRICS=true
      - RAY_METRICS_EXPORT_PORT=8080
      - RAY_GRAFANA_HOST=http://grafana:3000
      - RAY_PROMETHEUS_HOST=http://prometheus:9090
      - RAY_GRAFANA_IFRAME_HOST=http://localhost:3000
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, ray, status]
      timeout: 10s
    image: "rayproject/ray:${RAY_VERSION:-2.44.1-aarch64}"
    ports:
      - "${RAY_PORT:-8265}:8265"
      - "6379:6379"
      - "8080:8080"
    shm_size: 4gb
    ulimits:
      memlock: -1
      nofile:
        hard: 65536
        soft: 65536
  redash:
    command: scheduler
    container_name: redash-scheduler
    depends_on:
      postgres:
        condition: service_completed_successfully
      redash-server:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      PYTHONUNBUFFERED: 0
      QUEUES: "queries,scheduled_queries,celery"
      REDASH_COOKIE_SECRET: veryverysecret
      REDASH_DATABASE_URL: "postgresql://postgres:postgres@postgres/redash"
      REDASH_LOG_LEVEL: INFO
      REDASH_REDIS_URL: "redis://redis:6379/0"
      WORKERS_COUNT: 1
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD, /bin/sh, -c, /app/bin/docker-entrypoint workers_healthcheck]
      timeout: 5s
    image: "redash/redash:${REDASH_VERSION:-10.1.0.b50633}"
  redash-data:
    command: [/app/bin/docker-entrypoint, create_db]
    container_name: redash-data
    environment:
      PYTHONUNBUFFERED: 0
      REDASH_COOKIE_SECRET: veryverysecret
      REDASH_DATABASE_URL: "postgresql://postgres:postgres@postgres/redash"
      REDASH_LOG_LEVEL: INFO
      REDASH_REDIS_URL: "redis://redis:6379/0"
    image: "redash/redash:${REDASH_VERSION:-10.1.0.b50633}"
  redash-server:
    command: server
    container_name: redash
    depends_on:
      redash-data:
        condition: service_completed_successfully
    environment:
      PYTHONUNBUFFERED: 0
      REDASH_COOKIE_SECRET: veryverysecret
      REDASH_DATABASE_URL: "postgresql://postgres:postgres@postgres/redash"
      REDASH_LOG_LEVEL: INFO
      REDASH_REDIS_URL: "redis://redis:6379/0"
      REDASH_WEB_WORKERS: 4
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD, curl, --output, /dev/null, --silent, --head, --fail, http://localhost:5000/ping]
      timeout: 5s
    image: "redash/redash:${REDASH_VERSION:-10.1.0.b50633}"
    ports:
      - "5000:5000"
  redis:
    container_name: redis
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, redis-cli, --raw, incr, ping]
      timeout: 5s
    image: "redis:${REDIS_VERSION:-7.4.2}"
    ports:
      - "6379:6379"
  solace:
    container_name: solace-data
    depends_on:
      solace-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /opt/app/my_data.sh]
    image: "solace/solace-pubsub-standard:${SOLACE_VERSION:-10.12}"
    volumes:
      - "./data/solace:/opt/app"
  solace-server:
    container_name: solace
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 1
    environment:
      - username_admin_globalaccesslevel=admin
      - "username_admin_password=${SOLACE_PASSWORD:-admin}"
      - system_scaling_maxconnectioncount=100
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD, curl, --output, /dev/null, --silent, --head, --fail, "http://localhost:8080"]
      timeout: 5s
    image: "solace/solace-pubsub-standard:${SOLACE_VERSION:-10.12}"
    ports:
      - "8080:8080"
      - "55554:55555"
    shm_size: 1g
    ulimits:
      core: -1
      nofile:
        hard: 6592
        soft: 2448
  sonarqube:
    container_name: sonarqube
    environment:
      - SONAR_WEB_CONTEXT=/sonar
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, "wget --spider localhost:9000/sonar"]      
      timeout: 10s
    image: "sonarqube:${SONARQUBE_VERSION:-lts-community}"
    ports:
      - "9000:9000"
  spanner:
    container_name: spanner
    image: "gcr.io/cloud-spanner-emulator/emulator:${SPANNER_VERSION:-1.5.32}"
    ports:
      - "9010:9010"
      - "9020:9020"
  sqlite:
    command: [tail, -f, /dev/null]
    container_name: sqlite
    image: "keinos/sqlite3:${SQLITE_VERSION:-3.49.1}"
    volumes:
      - "./data/sqlite:/opt/data"
  superset:
    command: [/app/docker/docker-init.sh]
    container_name: superset_init
    depends_on:
      postgres:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      superset-server:
        condition: service_healthy
    env_file: ./data/superset/docker/.env
    healthcheck:
      disable: true
    image: "apache/superset:${SUPERSET_VERSION:-4.1.2}"
    user: root
    volumes:
      - "./data/superset/docker:/app/docker"
  superset-server:
    command: [/app/docker/docker-bootstrap.sh, app-gunicorn]
    container_name: superset
    env_file: ./data/superset/docker/.env
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, curl, --output, /dev/null, --silent, --head, --fail, "http://localhost:8088/health"]
      timeout: 5s
    image: "apache/superset:${SUPERSET_VERSION:-4.1.2}"
    ports:
      - "8088:8088"
    user: root
    volumes:
      - "./data/superset/docker:/app/docker"
  temporal:
    command: [server, start-dev, --db-filename, /opt/data/db/temporal.db, --ip, 0.0.0.0, --metrics-port, "9233"]
    container_name: temporal
    entrypoint: temporal
    environment: []
    expose:
      - 8233
      - 7233
    image: "temporalio/server:${TEMPORAL_VERSION:-1.27.2.0}"
    ports:
      - "8233:8233"
      - "7233:7233"
      - "9233:9233"
  timescaledb:
    container_name: timescaledb
    environment:
      - "POSTGRES_USER=${TIMESCALEDB_USER:-postgres}"
      - "POSTGRES_PASSWORD=${TIMESCALEDB_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, pg_isready -U postgres]
      timeout: 5s
    image: "timescale/timescaledb:${TIMESCALEDB_VERSION:-latest-pg15}"
    ports:
      - "${TIMESCALEDB_PORT:-5432}:5432"
  trino:
    container_name: trino
    depends_on:
      postgres:
        condition: service_completed_successfully
    image: "trinodb/trino:${TRINO_VERSION:-474}"
    ports:
      - "8084:8080"
    volumes:
      - "./data/trino/etc:/usr/lib/trino/etc:ro"
      - "./data/trino/catalog:/etc/trino/catalog"
  unitycatalog:
    container_name: unitycatalog
    image: "datacatering/unitycatalog:${UNITYCATALOG_VERSION:-0.1.0}"
    ports:
      - "8081:8081"
  vault:
    cap_add:
      - IPC_LOCK
    container_name: vault
    environment:
      - VAULT_DEV_ROOT_TOKEN_ID=root
      - VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, vault, status]
      timeout: 5s
    image: "hashicorp/vault:${VAULT_VERSION:-1.19}"
    ports:
      - "${VAULT_PORT:-8200}:8200"
  weaviate:
    container_name: weaviate
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      CLUSTER_HOSTNAME: node1
      DEFAULT_VECTORIZER_MODULE: none
      QUERY_DEFAULTS_LIMIT: 25
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, -f, "http://localhost:8080/v1/.well-known/ready"]
      timeout: 5s
    image: "semitechnologies/weaviate:${WEAVIATE_VERSION:-1.30.1}"
    ports:
      - "${WEAVIATE_PORT:-8080}:8080"
  zookeeper:
    container_name: zookeeper
    environment:
      - ZOO_MY_ID=1
    healthcheck:
      interval: 5s
      retries: 3
      test: "nc -z localhost 2181 || exit -1"
      timeout: 5s
    image: "zookeeper:${ZOOKEEPER_VERSION:-3.9.3}"
    ports:
      - "2181:2181"
