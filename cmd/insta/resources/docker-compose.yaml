services:
  activemq:
    container_name: activemq
    environment:
      - "ARTEMIS_USER=${ARTEMIS_USER:-artemis}"
      - "ARTEMIS_PASSWORD=${ARTEMIS_PASSWORD:-artemis}"
    healthcheck:
      interval: 15s
      retries: 3
      test: [CMD-SHELL, "curl -k -f http://localhost:8161/"]
      timeout: 5s
    image: "apache/activemq-artemis:${ACTIVEMQ_VERSION:-2.42.0}"
    ports:
      - "61616:61616"
      - "8161:8161"
  airflow:
    command: standalone
    container_name: airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file: ./data/airflow/.env
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 30s
      test: [CMD, curl, --fail, "http://localhost:8080/api/v2/monitor/health"]
      timeout: 10s
    image: "apache/airflow:${AIRFLOW_VERSION:-3.1.0}"
    ports:
      - "8081:8080"
    restart: always
    user: "50000:0"
    volumes:
      - "./data/airflow/dags:/opt/airflow/dags"
      - "./data/airflow/passwords.json:/opt/airflow/passwords.json"
  airflow-init:
    command: [-c, /tmp/scripts/init.sh]
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: /bin/bash
    env_file: ./data/airflow/.env
    image: "apache/airflow:${AIRFLOW_VERSION:-3.1.0}"
    user: "0:0"
    volumes:
      - "./data/airflow/init.sh:/tmp/scripts/init.sh"
  amundsen:
    command: gunicorn -w 2 --bind :5000 amundsen_application.wsgi
    container_name: amundsen
    depends_on:
      - amundsen-metadata
      - amundsen-search
    environment:
      - SEARCHSERVICE_BASE=http://amundsen-search:5000
      - METADATASERVICE_BASE=http://amundsen-metadata:5000
      - FRONTEND_SVC_CONFIG_MODULE_CLASS=amundsen_application.config.TestConfig
    image: "amundsendev/amundsen-frontend:${AMUNDSEN_FRONTEND_VERSION:-4.3.0}"
    ports:
      - "5003:5000"
  amundsen-metadata:
    command: gunicorn -w 2 --bind :5000 metadata_service.metadata_wsgi
    container_name: amundsen-metadata
    depends_on:
      - amundsen-neo4j
    environment:
      - PROXY_HOST=bolt://amundsen-neo4j
      - PROXY_ENCRYPTED=True
      - PROXY_VALIDATE_SSL=False
    image: "amundsendev/amundsen-metadata:${AMUNDSEN_METADATA_VERSION:-3.13.0}"
    ports:
      - "5002:5000"
  amundsen-neo4j:
    container_name: amundsen-neo4j
    environment:
      - NEO4J_AUTH=none
    image: "datacatering/neo4j:3.5.35"
    ports:
      - "7474:7474"
      - "7687:7687"
    ulimits:
      nofile:
        hard: 40000
        soft: 40000
    volumes:
      - "./data/amundsen-neo4j/conf:/var/lib/neo4j/conf"
  amundsen-search:
    command: gunicorn -w 2 --bind :5000 search_service.search_wsgi
    container_name: amundsen-search
    depends_on:
      - elasticsearch
    environment:
      - PROXY_ENDPOINT=elasticsearch
    image: "amundsendev/amundsen-search:${AMUNDSEN_SEARCH_VERSION:-4.2.0}"
    ports:
      - "5001:5000"
  argilla:
    command: [/data/argilla_scripts/run_all_argilla_loaders.sh]
    container_name: argilla-data
    depends_on:
      - argilla-server
    environment:
      - ARGILLA_API_URL=http://argilla:6900
      - ARGILLA_API_KEY=${ARGILLA_API_KEY:-argilla.apikey}
      - ARGILLA_WORKSPACE=${ARGILLA_WORKSPACE:-default}
      - PYTHONUNBUFFERED=1
    image: "argilla/argilla-server:${ARGILLA_VERSION:-v2.8.0}"
    restart: no
    volumes:
      - "./data/argilla:/data/argilla_scripts"
  argilla-server:
    container_name: argilla
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
      postgres:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    environment:
      - ARGILLA_ELASTICSEARCH=http://argilla_user:${ELASTICSEARCH_ARGILLA_PASSWORD:-PleaseChangeMeArgilla}@elasticsearch:9200
      - ARGILLA_REDIS_URL=redis://redis:6379/0
      - ARGILLA_DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${ARGILLA_DB_NAME:-argilla}
      - USERNAME=${ARGILLA_USER:-argilla}
      - PASSWORD=${ARGILLA_PASSWORD:-12345678}
      - WORKSPACE=default
      - API_KEY=argilla.apikey
    image: "argilla/argilla-server:${ARGILLA_VERSION:-v2.8.0}"
    ports:
      - "${ARGILLA_PORT:-6900}:6900"
  blazer:
    command: sh -c "rails db:migrate && puma -C /app/config/puma.rb"
    container_name: blazer
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - DATABASE_URL=postgres://postgres:postgres@postgres:5432/blazer
      - DATABASE_CUSTOMER_URL=postgres://postgres:postgres@postgres:5432/customer
    image: "ankane/blazer:${BLAZER_VERSION:-v3.3.0}"
    ports:
      - "8080:8080"
    volumes:
      - "./data/blazer/blazer.yml:/app/config/blazer.yml"
  cassandra:
    command: [-c, /tmp/scripts/init.sh]
    container_name: cassandra-data
    depends_on:
      cassandra-server:
        condition: service_healthy
    entrypoint: /bin/bash
    environment:
      - DS_LICENSE=accept
    image: "datacatering/dse-server:${CASSANDRA_VERSION:-6.8.48}"
    volumes:
      - "./data/cassandra/init.sh:/tmp/scripts/init.sh"
      - "${CASSANDRA_DATA:-./data/cassandra/data}:/tmp/data"
  cassandra-server:
    cap_add:
      - IPC_LOCK
    container_name: cassandra
    environment:
      - DS_LICENSE=accept
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD-SHELL, "[ $$(nodetool statusgossip) = running ]"]
      timeout: 10s
    image: "datacatering/dse-server:${CASSANDRA_VERSION:-6.8.48}"
    ports:
      - "9042:9042"
    ulimits:
      memlock: -1
  clickhouse:
    command: [/bin/bash, -c, /tmp/scripts/init.sh]
    container_name: clickhouse-data
    depends_on:
      clickhouse-server:
        condition: service_healthy
    hostname: clickhouse
    image: "clickhouse/clickhouse-server:${CLICKHOUSE_VERSION:-25.9.3}"
    user: "101:101"
    volumes:
      - "./data/clickhouse/init.sh:/tmp/scripts/init.sh"
      - "${CLICKHOUSE_DATA:-./data/clickhouse/data}:/tmp/data"
  clickhouse-server:
    container_name: clickhouse
    depends_on:
      postgres:
        condition: service_completed_successfully
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1"
      timeout: 5s
    hostname: clickhouse
    image: "clickhouse/clickhouse-server:${CLICKHOUSE_VERSION:-25.9.3}"
    ports:
      - "8123:8123"
      - "9000:9000"
    user: "101:101"
  cockroachdb:
    command: [bash, -c, /tmp/scripts/init.sh]
    container_name: cockroachdb-data
    depends_on:
      cockroachdb-server:
        condition: service_healthy
    image: "cockroachdb/cockroach:${COCKROACHDB_VERSION:-v25.3.2}"
    volumes:
      - "./data/cockroachdb/init.sh:/tmp/scripts/init.sh"
      - "${COCKROACHDB_DATA:-./data/cockroachdb/data}:/tmp/data"
  cockroachdb-server:
    command: [start-single-node, --insecure]
    container_name: cockroachdb
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD-SHELL, "curl --fail http://localhost:8080/ || exit 1"]
      timeout: 5s
    image: "cockroachdb/cockroach:${COCKROACHDB_VERSION:-v25.3.2}"
    ports:
      - "26257:26257"
      - "8080:8080"
  confluent-schema-registry:
    container_name: confluent-schema-registry
    depends_on:
      kafka-server:
        condition: service_healthy
    env_file: ./data/confluent-schema-registry/env/docker.env
    healthcheck:
      interval: 10s
      retries: 5
      test: nc -z confluent-schema-registry 8081
      timeout: 5s
    hostname: confluent-schema-registry
    image: confluentinc/cp-schema-registry:${CONFLUENT_SCHEMA_REGISTRY_VERSION:-7.4.0}
    ports:
      - "8081:8081"
  cvat:
    container_name: cvat-traefik
    depends_on:
      cvat-data:
        condition: service_completed_successfully
      cvat-ui:
        condition: service_healthy
    env_file: ./data/cvat/.traefik_env
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, traefik, healthcheck]
      timeout: 3s
    image: traefik:${TRAEFIK_VERSION:-v3.4.0}
    ports:
      - 8080:8080
      - 8090:8090
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
  cvat-data:
    container_name: cvat-data
    depends_on:
      cvat-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /tmp/scripts/init.sh]
    env_file: ./data/cvat/.env
    environment:
      - "CVAT_USER=${CVAT_USER:-admin}"
      - "CVAT_PASSWORD=${CVAT_PASSWORD:-admin}"
      - "CVAT_EMAIL=${CVAT_EMAIL:-admin@example.com}"
    image: "cvat/server:${CVAT_VERSION:-v2.47.0}"
    volumes:
      - "./data/cvat/init.sh:/tmp/scripts/init.sh"
  cvat-opa:
    command: [run, --server, --log-level=error, --set=services.cvat.url=http://cvat-server:8080, --set=bundles.cvat.service=cvat, --set=bundles.cvat.resource=/api/auth/rules, --set=bundles.cvat.polling.min_delay_seconds=5, --set=bundles.cvat.polling.max_delay_seconds=15]
    container_name: opa
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, opa, eval, "true"]
      timeout: 3s
    image: "openpolicyagent/opa:${OPA_VERSION:-1.9.0}"
  cvat-server:
    command: init run server
    container_name: cvat-server
    depends_on:
      cvat-opa:
        condition: service_healthy
      postgres:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    env_file: ./data/cvat/.env
    healthcheck:
      interval: 5s
      retries: 5
      start_period: 10s # Give time for DB migrations and startup
      test: [CMD-SHELL, python manage.py health_check]
      timeout: 3s
    image: "cvat/server:${CVAT_VERSION:-v2.47.0}"
    labels:
      traefik.enable: "true"
      traefik.http.routers.cvat.entrypoints: web
      traefik.http.routers.cvat.rule:
        Host(`${CVAT_HOST:-localhost}`) &&
        (PathPrefix(`/api/`) || PathPrefix(`/static/`) || PathPrefix(`/admin`)
        || PathPrefix(`/django-rq`))
      traefik.http.services.cvat.loadbalancer.server.port: "8080"
    volumes:
      - "./data/cvat/data:/home/django/data"
      - "./data/cvat/keys:/home/django/keys"
      - "./data/cvat/logs:/home/django/logs"
      - "./data/cvat/models:/home/django/models"
      - "./data/cvat/share:/mnt/share:ro" # Read-only mount for shared datasets
  cvat-ui:
    container_name: cvat-ui
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, nginx, -t]
      timeout: 5s
    image: "cvat/ui:${CVAT_VERSION:-v2.47.0}"
    labels:
      traefik.enable: "true"
      traefik.http.routers.cvat-ui.entrypoints: web
      traefik.http.routers.cvat-ui.rule: Host(`${CVAT_HOST:-localhost}`)
      traefik.http.services.cvat-ui.loadbalancer.server.port: "80"
  dagster:
    container_name: dagster
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: [dagster-webserver, -h, 0.0.0.0, -p, "3000", -w, /opt/dagster/app/workspace.yaml]
    environment:
      - DAGSTER_POSTGRES_HOST=postgres
      - "DAGSTER_POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "DAGSTER_POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
      - DAGSTER_POSTGRES_DB=dagster
      - DAGSTER_HOME=/opt/dagster/dagster_home/
    image: "dagster/dagster-k8s:${DAGSTER_VERSION:-1.11.15}"
    ports:
      - "3000:3000"
    volumes:
      - "./data/dagster:/opt/dagster/app/"
  data-caterer:
    container_name: data-caterer
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - DEPLOY_MODE=standalone
    image: "datacatering/data-caterer:${DATA_CATERER_VERSION:-0.16.11}"
    ports:
      - "9898:9898"
    volumes:
      - "./data/data-caterer/connection:/opt/DataCaterer/connection"
      - "./data/data-caterer/plan:/opt/DataCaterer/plan"
  datahub:
    container_name: datahub
    depends_on:
      datahub-gms:
        condition: service_healthy
    env_file: ./data/datahub-frontend/env/docker.env
    environment:
      - ELASTIC_CLIENT_USERNAME=elastic
      - "ELASTIC_CLIENT_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    hostname: datahub
    image: acryldata/datahub-frontend-react:${DATAHUB_VERSION:-head}
    ports:
      - "9002:9002"
  datahub-actions:
    container_name: datahub-actions
    depends_on:
      datahub-gms:
        condition: service_healthy
    env_file: ./data/datahub-actions/env/docker.env
    environment:
      - ACTIONS_EXTRA_PACKAGES=${ACTIONS_EXTRA_PACKAGES:-}
      - ACTIONS_CONFIG=${ACTIONS_CONFIG:-}
    hostname: actions
    image: acryldata/datahub-actions:${DATAHUB_VERSION:-head}
  datahub-gms:
    container_name: datahub-gms
    depends_on:
      datahub-upgrade:
        condition: service_completed_successfully
    env_file: ./data/datahub-upgrade/env/docker-without-neo4j.env
    environment:
      - KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR=${KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR:-true}
      - EBEAN_DATASOURCE_USERNAME=root
      - "EBEAN_DATASOURCE_PASSWORD=${MYSQL_PASSWORD:-root}"
      - ELASTICSEARCH_USERNAME=elastic
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    healthcheck:
      interval: 1s
      retries: 3
      start_period: 90s
      test: "curl -sS --fail http://datahub-gms:8080/health"
      timeout: 5s
    hostname: datahub-gms
    image: acryldata/datahub-gms:${DATAHUB_VERSION:-head}
    ports:
      - "8080:8080"
  datahub-kafka-setup:
    container_name: datahub-kafka-setup
    depends_on:
      confluent-schema-registry:
        condition: service_healthy
      kafka-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /tmp/scripts/init.sh]
    environment:
      - "KAFKA_TOPICS=${KAFKA_TOPICS:-MetadataAuditEvent_v4,MetadataChangeEvent_v4,FailedMetadataChangeEvent_v4,MetadataChangeLog_Versioned_v1,MetadataChangeLog_Timeseries_v1,MetadataChangeProposal_v1,FailedMetadataChangeProposal_v1,PlatformEvent_v1,DataHubUpgradeHistory_v1}"
    image: "confluentinc/confluent-local:${KAFKA_VERSION:-8.1.0}"
    volumes:
      - "./data/kafka/init.sh:/tmp/scripts/init.sh"
  datahub-upgrade:
    command:
      - -u
      - SystemUpdate
    container_name: datahub-upgrade
    depends_on:
      datahub-kafka-setup:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_completed_successfully
      mysql:
        condition: service_completed_successfully
      neo4j:
        condition: service_healthy
    env_file: ./data/datahub-upgrade/env/docker-without-neo4j.env
    environment:
      - EBEAN_DATASOURCE_USERNAME=root
      - "EBEAN_DATASOURCE_PASSWORD=${MYSQL_PASSWORD:-root}"
      - ELASTICSEARCH_USERNAME=elastic
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    hostname: datahub-upgrade
    image: acryldata/datahub-upgrade:${DATAHUB_VERSION:-head}
    labels:
      datahub_setup_job: true
  debezium:
    container_name: debezium
    depends_on:
      debezium-connect:
        condition: service_healthy
    environment:
      - "KAFKA_CONNECT_URIS=http://debezium-connect:8083"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8080"]
      timeout: 10s
    image: "debezium/debezium-ui:${DEBEZIUM_VERSION:-2.1.2.Final}"
    ports:
      - "8080:8080"
  debezium-connect:
    container_name: debezium-connect
    depends_on:
      - kafka
    env_file: ./data/debezium/.env
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8083"]
      timeout: 10s
    image: "debezium/connect:${DEBEZIUM_CONNECT_VERSION:-2.6.2.Final}"
    ports:
      - "8083:8083"
  doccano:
    command: sh -c "pip install --no-cache-dir doccano-client==1.2.8 requests requests-toolbelt==0.9.1 urllib3==1.26.5 && python /scripts/load_doccano_data.py"
    container_name: doccano-data
    depends_on:
      doccano-server:
        condition: service_healthy
    environment:
      DOCCANO_ADMIN_PASSWORD: "${ADMIN_PASSWORD:-admin}"
      DOCCANO_ADMIN_USERNAME: "${ADMIN_USERNAME:-admin}"
      DOCCANO_URL: "http://doccano-server:8000"
    image: python:3.14-slim
    restart: no
    volumes:
      - "./data/doccano/load_doccano_data.py:/scripts/load_doccano_data.py:ro"
      - "./data/doccano/sample_data.jsonl:/data/sample_data.jsonl:ro"
  doccano-server:
    container_name: doccano
    environment:
      ADMIN_EMAIL: "${ADMIN_EMAIL:-admin@example.com}"
      ADMIN_PASSWORD: "${ADMIN_PASSWORD:-admin}"
      ADMIN_USERNAME: "${ADMIN_USERNAME:-admin}"
    healthcheck:
      interval: 5s
      retries: 5
      start_period: 5s
      test: [CMD, python, /opt/healthcheck/doccano_healthcheck.py]
      timeout: 5s
    image: "doccano/doccano:${DOCCANO_VERSION:-1.8.4}"
    ports:
      - "${DOCCANO_PORT:-8000}:8000"
    volumes:
      - "./data/doccano/doccano_healthcheck.py:/opt/healthcheck/doccano_healthcheck.py:ro"
  doris:
    container_name: doris
    depends_on:
      postgres:
        condition: service_completed_successfully
    image: "apache/doris:${DORIS_VERSION:-doris-all-in-one-2.1.0}"
    ports:
      - "8030:8030"
      - "8040:8040"
      - "9030:9030"
  druid:
    command: [router]
    container_name: druid
    depends_on:
      druid-broker:
        condition: service_healthy
      druid-coordinator:
        condition: service_healthy
      druid-historical:
        condition: service_healthy
      druid-middlemanager:
        condition: service_healthy
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8888/status/health || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-34.0.0}"
    ports:
      - "8888:8888"
  druid-broker:
    command: [broker]
    container_name: druid-broker
    depends_on:
      druid-coordinator:
        condition: service_healthy
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8082/druid/broker/v1/loadstatus || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-34.0.0}"
    ports:
      - "8082:8082"
  druid-coordinator:
    command: [coordinator]
    container_name: druid-coordinator
    depends_on:
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8081/status/health || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-34.0.0}"
    ports:
      - "8081:8081"
  druid-historical:
    command: [historical]
    container_name: druid-historical
    depends_on:
      druid-coordinator:
        condition: service_healthy
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8083/druid/historical/v1/readiness || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-34.0.0}"
    ports:
      - "8083:8083"
  druid-middlemanager:
    command: [middleManager]
    container_name: druid-middlemanager
    depends_on:
      druid-coordinator:
        condition: service_healthy
      postgres:
        condition: service_completed_successfully
      zookeeper:
        condition: service_healthy
    env_file: ./data/druid/environment
    environment:
      - "druid_metadata_storage_connector_user=${POSTGRES_USER:-postgres}"
      - "druid_metadata_storage_connector_password=${POSTGRES_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: "wget --no-verbose --tries=1 --spider http://localhost:8091/status/health || exit 1"
      timeout: 5s
    image: "apache/druid:${DRUID_VERSION:-34.0.0}"
    ports:
      - "8091:8091"
      - "8100-8105:8100-8105"
  duckdb:
    container_name: duckdb
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: [tail, -F, anything]
    image: "datacatering/duckdb:${DUCKDB_VERSION:-v1.4.1}"
    volumes:
      - "./data/duckdb:/opt/data"
  elasticsearch:
    container_name: elasticsearch-data
    depends_on:
      elasticsearch-server:
        condition: service_healthy
    entrypoint: /tmp/entrypoint.sh
    env_file: ./data/elasticsearch/.env
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_VERSION:-9.1.5}"
    volumes:
      - "./data/elasticsearch/data/entrypoint.sh:/tmp/entrypoint.sh"
      - "./data/elasticsearch/data/lib.sh:/tmp/lib.sh"
      - "./data/elasticsearch/data/roles:/tmp/roles"
  elasticsearch-server:
    container_name: elasticsearch
    env_file: ./data/elasticsearch/.env
    healthcheck:
      interval: 10s
      retries: 5
      test: "curl -sS --fail http://elasticsearch:9200/_cluster/health?wait_for_status=yellow&timeout=0s"
      timeout: 5s
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_VERSION:-9.1.5}"
    ports:
      - "9200:9200"
      - "9300:9300"
    restart: unless-stopped
    volumes:
      - "./data/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro,Z"
  evidence:
    container_name: evidence
    image: "evidencedev/devenv:${EVIDENCE_VERSION:-latest}"
    init: true
    ports:
      - "3000:3000"
    volumes:
      - "./data/evidence/persist:/evidence-workspace"
  feast:
    command: [feast, -c, /feature_repo, serve, -h, 0.0.0.0]
    container_name: feast
    healthcheck:
      interval: 5s
      retries: 3
      test: "curl -sS --fail http://localhost:6566/health"
      timeout: 10s
    image: "feastdev/feature-server:${FEAST_VERSION:-0.46.0}"
    ports:
      - "${FEAST_PORT:-6566}:6566"
    volumes:
      - "./data/feast/feature_repo:/feature_repo"
  flight-sql:
    command: [tail, -f, /dev/null]
    container_name: flight-sql
    depends_on:
      - duckdb
      - sqlite
    environment:
      - TLS_ENABLED=1
      - "FLIGHT_PASSWORD=${FLIGHT_SQL_PASSWORD:-flight_password}"
      - PRINT_QUERIES=1
    image: "voltrondata/flight-sql:${FLIGHT_SQL_VERSION:-v1.4.1}"
    ports:
      - "31337:31337"
  flink:
    command: taskmanager
    container_name: flink
    depends_on:
      - flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - "FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager"
    expose:
      - 6121
      - 6122
    image: "flink:${FLINK_VERSION:-2.1.0-scala_2.12-java17}"
    links: []
  flink-jobmanager:
    command: jobmanager
    container_name: flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - "FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager"
    expose:
      - 6123
    image: "flink:${FLINK_VERSION:-2.1.0-scala_2.12-java17}"
    ports:
      - "8081:8081"
  fluentd:
    container_name: fluentd
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
    image: "datacatering/fluentd-elasticsearch:${FLUENTD_VERSION:-v1.17.0-debian-1.0}"
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    volumes:
      - "./data/fluentd/etc/fluent.conf:/fluentd/etc/fluent.conf"
  grafana:
    container_name: grafana
    env_file: ./data/grafana/.env
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      timeout: 5s
    image: "grafana/grafana:${GRAFANA_VERSION:-12.2.0}"
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - "./data/grafana/dashboards:/etc/grafana/dashboards"
      - "./data/grafana/provisioning:/etc/grafana/provisioning"
  httpbin:
    container_name: http
    environment:
      - "GUNICORN_CMD_ARGS=--capture-output --error-logfile - --access-logfile - --access-logformat '%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s'"
    image: "kennethreitz/httpbin:${HTTPBIN_VERSION:-latest}"
    ports:
      - "8080:80"
  httpd:
    container_name: httpd
    depends_on:
      - fluentd
    image: "httpd:${HTTPD_VERSION:-2.4.65}"
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        fluentd-async-connect: "true"
        tag: httpd.access
    ports:
      - "8080:80"
  influxdb:
    container_name: influxdb
    environment:
      - "DOCKER_INFLUXDB_INIT_USERNAME=${INFLUXDB_USER:-admin}"
      - "DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUXDB_PASSWORD:-admin}"
      - DOCKER_INFLUXDB_INIT_ORG=myorg
      - DOCKER_INFLUXDB_INIT_BUCKET=mybucket
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8086/health"]
      timeout: 5s
    image: "influxdb:${INFLUXDB_VERSION:-2.7}"
    ports:
      - "${INFLUXDB_PORT:-8086}:8086"
  jaeger:
    container_name: jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, wget, --no-verbose, --tries=1, --spider, "http://localhost:16686"]
      timeout: 5s
    image: "jaegertracing/all-in-one:${JAEGER_VERSION:-1.60}"
    ports:
      - "${JAEGER_PORT:-16686}:16686"
      - "14250:14250"
      - "14268:14268"
      - "6831:6831/udp"
  jupyter:
    command: [jupyter, notebook, --no-browser, "--NotebookApp.token=''", "--NotebookApp.password=''"]
    container_name: jupyter
    image: "quay.io/jupyter/minimal-notebook:${JUPYTER_VERSION:-2024-07-02}"
    ports:
      - "8888:8888"
    volumes:
      - "./data/jupyter:/home/jovyan/work"
  kafka:
    container_name: kafka-data
    depends_on:
      kafka-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /tmp/scripts/init.sh]
    environment:
      - "KAFKA_TOPICS=${KAFKA_TOPICS:-accounts,transactions}"
    image: "confluentinc/confluent-local:${KAFKA_VERSION:-8.1.0}"
    volumes:
      - "./data/kafka/init.sh:/tmp/scripts/init.sh"
  kafka-server:
    container_name: kafka
    environment:
      - "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - "KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092"
      - "KAFKA_LISTENERS=PLAINTEXT://kafka:29092,CONTROLLER://localhost:29093,PLAINTEXT_HOST://0.0.0.0:9092"
    expose:
      - 29092
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD-SHELL, /bin/sh, -c, kafka-topics, --bootstrap-server, "kafka:29092", --list]
      timeout: 5s
    image: "confluentinc/confluent-local:${KAFKA_VERSION:-8.1.0}"
    ports:
      - "9092:9092"
  keycloak:
    command: [start-dev, --metrics-enabled=true, --health-enabled=true, --import-realm, --verbose, --log-level=INFO, --features=token-exchange]
    container_name: keycloak
    depends_on:
      postgres:
        condition: service_completed_successfully
    env_file: ./data/keycloak/.env
    image: "quay.io/keycloak/keycloak:${KEYCLOACK_VERSION:-26.4.1}"
    ports:
      - "8082:8080"
    restart: unless-stopped
    volumes:
      - "./data/keycloak/realm.json:/opt/keycloak/data/import/realm.json:ro"
  kibana:
    container_name: kibana
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - "KIBANA_SYSTEM_PASSWORD=${KIBANA_SYSTEM_PASSWORD:-password}"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:5601/api/status"]
      timeout: 5s
    image: "docker.elastic.co/kibana/kibana:${KIBANA_VERSION:-9.1.5}"
    ports:
      - "5601:5601"
    restart: always
    volumes:
      - "./data/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml"
  kong:
    container_name: kong
    depends_on:
      kong-data:
        condition: service_completed_successfully
      postgres:
        condition: service_completed_successfully
    env_file: ./data/kong/.env
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD-SHELL, kong health]
      timeout: 10s
    image: "kong:${KONG_VERSION:-3.9.1}"
    ports:
      - 8001:8001
      - 8000:8000
    restart: unless-stopped
  kong-data:
    command:
      - kong
      - migrations
      - bootstrap
    container_name: kong-data
    depends_on:
      postgres:
        condition: service_completed_successfully
    env_file: ./data/kong/.env
    image: "kong:${KONG_VERSION:-3.9.1}"
  label-studio:
    container_name: label-studio
    image: heartexlabs/label-studio:${LABEL_STUDIO_VERSION:-1.18.0}
    ports:
      - "8080:8080"
  lakekeeper:
    command: [serve]
    container_name: lakekeeper
    depends_on:
      lakekeeper-mc:
        condition: service_completed_successfully
      lakekeeper-migrate:
        condition: service_completed_successfully
    env_file: ./data/lakekeeper/base/.env
    healthcheck:
      interval: 1s
      retries: 3
      start_period: 3s
      test: [CMD, /home/nonroot/iceberg-catalog, healthcheck]
      timeout: 10s
    image: quay.io/lakekeeper/catalog:${LAKEKEEPER_IMAGE:-latest-main}
    ports:
      - "8183:8181"
  lakekeeper-jupyter:
    command: start-notebook.sh --NotebookApp.token=''
    container_name: lakekeeper-jupyter
    depends_on:
      lakekeeper:
        condition: service_healthy
      lakekeeper-trino:
        condition: service_healthy
      lakekeeper-trino-proxy:
        condition: service_healthy
    image: quay.io/jupyter/pyspark-notebook:${LAKEKEEPER_JUPYTER_VERSION:-2024-10-14}
    ports:
      - "8889:8888"
    volumes:
      - ./data/lakekeeper/access-control-advanced/notebooks:/home/jovyan/examples/
  lakekeeper-mc:
    container_name: lakekeeper-mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 minioadmin minioadmin) do echo '...waiting...' && sleep 5; done;
      /usr/bin/mc mb minio/examples;
      /usr/bin/mc anonymous set public minio/examples;
      " 
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    image: minio/mc
    restart: no
  lakekeeper-migrate:
    container_name: lakekeeper-migrate
    depends_on:
      keycloak:
        condition: service_healthy
      lakekeeper-openfga:
        condition: service_healthy
    entrypoint: [/home/nonroot/iceberg-catalog, migrate]
    env_file: ./data/lakekeeper/migrate/.env
    image: quay.io/lakekeeper/catalog:${LAKEKEEPER_IMAGE_VERSION:-latest-main}
    pull_policy: if_not_present
    restart: no
  lakekeeper-opa:
    command: [run, --server, --log-format, text, --log-level, debug, --addr, "0.0.0.0:8181", /policies]
    container_name: lakekeeper-opa
    depends_on:
      lakekeeper:
        condition: service_healthy
    env_file: ./data/opa/.env
    image: openpolicyagent/opa:${OPA_VERSION:-1.0.0}
    volumes:
      - ./data/opa/policies:/policies
  lakekeeper-openfga:
    command: run
    container_name: lakekeeper-openfga
    depends_on:
      lakekeeper-openfga-migrate:
        condition: service_completed_successfully
    env_file: ./data/lakekeeper/access-control-advanced/openfga/.env
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, /usr/local/bin/grpc_health_probe, "-addr=lakekeeper-openfga:8081"]
      timeout: 30s
    image: openfga/openfga:${LAKEKEEPER_OPENFGA_VERSION:-v1.8}
  lakekeeper-openfga-migrate:
    command: migrate
    container_name: lakekeeper-openfga-migrate
    depends_on:
      postgres:
        condition: service_completed_successfully
    environment:
      - OPENFGA_DATASTORE_ENGINE=postgres
      - OPENFGA_DATASTORE_URI=postgres://postgres:postgres@postgres:5432/lakekeeper
    image: openfga/openfga:${LAKEKEEPER_OPENFGA_VERSION:-v1.8}
  lakekeeper-trino:
    container_name: lakekeeper-trino
    depends_on:
      lakekeeper-opa:
        condition: service_started
    env_file: ./data/lakekeeper/access-control-advanced/trino/.env
    healthcheck:
      interval: 2s
      retries: 2
      start_period: 10s
      test: [CMD, curl, -I, "http://localhost:8080/v1/status"]
      timeout: 10s
    image: trinodb/trino:${LAKEKEEPER_TRINO_VERSION:-467}
    ports:
      - "8880:8080"
    volumes:
      - ./data/lakekeeper/access-control-advanced/trino/config/access-control.properties:/etc/trino/access-control.properties
      - ./data/lakekeeper/access-control-advanced/trino/config/config.properties:/etc/trino/config.properties
      - ./data/lakekeeper/access-control-advanced/trino/config/log.properties:/etc/trino/log.properties
  lakekeeper-trino-proxy:
    container_name: lakekeeper-trino-proxy
    depends_on:
      lakekeeper-trino:
        condition: service_healthy
    healthcheck:
      interval: 1s
      retries: 3
      start_period: 3s    
      test: [CMD, service, nginx, status]
      timeout: 10s
    image: nginx
    ports:
      - 443:443
      - 38191:38191
    volumes:
      - ./data/lakekeeper/access-control-advanced/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./data/lakekeeper/access-control-advanced/nginx/certs:/etc/nginx/certs
  logstash:
    container_name: logstash
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
    environment:
      LOGSTASH_INTERNAL_PASSWORD: "${LOGSTASH_INTERNAL_PASSWORD:-password}"
      LS_JAVA_OPTS: -Xms256m -Xmx256m
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:9600"]
      timeout: 5s
    image: "docker.elastic.co/logstash/logstash:${LOGSTASH_VERSION:-9.1.5}"
    ports:
      - "5044:5044"
      - "50000:50000/tcp"
      - "50000:50000/udp"
      - "9600:9600"
    restart: unless-stopped
    volumes:
      - "./data/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml"
      - "./data/logstash/pipeline:/usr/share/logstash/pipeline"
  loki:
    command: -config.file=/etc/loki/local-config.yaml
    container_name: loki
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      timeout: 5s
    image: "grafana/loki:${LOKI_VERSION:-3.5.7}"
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - "./data/loki/config.yaml:/etc/loki/local-config.yaml"
  maestro:
    container_name: maestro
    depends_on:
      cockroachdb:
        condition: service_completed_successfully
    environment:
      - CONDUCTOR_CONFIGS_JDBCURL=jdbc:postgresql://cockroachdb:26257/maestro
      - CONDUCTOR_CONFIGS_JDBCUSERNAME=root
    image: "datacatering/maestro:${MAESTRO_VERSION:-0.1.0}"
    ports:
      - "8081:8080"
  mage-ai:
    command: mage start your_first_project
    container_name: mage-ai
    environment:
      - USER_CODE_PATH=/home/src/your_first_project
    image: "mageai/mageai:${MAGE_AI_VERSION:-0.9.78}"
    ports:
      - "6789:6789"
    restart: on-failure
  mariadb:
    container_name: mariadb
    environment:
      - "MARIADB_USER=${MARIADB_USER:-user}"
      - "MARIADB_PASSWORD=${MARIADB_PASSWORD:-password}"
      - MARIADB_ROOT_PASSWORD=root
      - MARIADB_DATABASE=customer
    image: "mariadb:${MARIADB_VERSION:-12.0.2}"
    ports:
      - "3306:3306"
    restart: always
  marquez:
    container_name: marquez-web
    depends_on:
      - marquez-data
    environment:
      - MARQUEZ_HOST=host.docker.internal
      - MARQUEZ_PORT=5002
    image: "marquezproject/marquez-web:${MARQUEZ_VERSION:-0.51.1}"
    ports:
      - "3001:3000"
  marquez-data:
    command: [-c, /tmp/scripts/init.sh]
    container_name: marquez-data
    depends_on:
      marquez-server:
        condition: service_healthy
    entrypoint: /bin/bash
    environment:
      - "MARQUEZ_URL=http://marquez:5000"
    image: "marquezproject/marquez:${MARQUEZ_VERSION:-0.51.1}"
    volumes:
      - "./data/marquez/init.sh:/tmp/scripts/init.sh"
      - "${MARQUEZ_DATA:-./data/marquez/data}:/tmp/data"
  marquez-server:
    container_name: marquez
    depends_on:
      postgres:
        condition: service_completed_successfully
    env_file: ./data/marquez/.env
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:5001/healthcheck"]
      timeout: 5s
    image: "marquezproject/marquez:${MARQUEZ_VERSION:-0.51.1}"
    ports:
      - "5002:5000"
      - "5001:5001"
    volumes:
      - "./data/marquez/conf:/opt/app"
  metabase:
    container_name: metabase
    depends_on:
      postgres:
        condition: service_completed_successfully
    env_file: ./data/metabase/.env
    healthcheck:
      interval: 15s
      retries: 5
      test: [CMD-SHELL, curl, --fail, -I, http://localhost:3000/api/health || exit 1]
      timeout: 5s
    image: "metabase/metabase:${METABASE_VERSION:-v0.56.10}"
    ports:
      - "3000:3000"
    volumes:
      - /dev/urandom:/dev/random:ro
  milvus:
    command: [standalone]
    container_name: milvus
    environment:
      - ETCD_ENDPOINTS=localhost:2379
      - MINIO_ADDRESS=localhost
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD, curl, -f, "http://localhost:9091/healthz"]
      timeout: 10s
    image: "milvusdb/milvus:${MILVUS_VERSION:-v2.6.3}"
    ports:
      - "${MILVUS_PORT:-19530}:19530"
      - "9091:9091"
  minio:
    command: [server, /data, --console-address, ":9001"]
    container_name: minio
    environment:
      - "MINIO_ROOT_USER=${MINIO_USER:-minioadmin}"
      - "MINIO_ROOT_PASSWORD=${MINIO_PASSWORD:-minioadmin}"
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, mc, ready, local]
      timeout: 5s
    image: "quay.io/minio/minio:${MINIO_VERSION:-RELEASE.2024-06-04T19-20-08Z}"
    ports:
      - "9000:9000"
      - "9001:9001"
  mlflow:
    command: mlflow server --host 0.0.0.0
    container_name: mlflow
    depends_on:
      jupyter:
        condition: service_healthy
    environment:
      - "MLFLOW_TRACKING_URI=http://localhost:5000"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, python, -c, "import urllib.request; urllib.request.urlopen('http://localhost:5000/health')"]
      timeout: 5s
    image: "ghcr.io/mlflow/mlflow:${MLFLOW_VERSION:-v3.5.0}"
    ports:
      - "${MLFLOW_PORT:-5000}:5000"
  mlflow-serve:
    command: mlflow models serve -m 'models:/diabetes-predictor/latest' -h 0.0.0.0 -p 5001 --no-conda
    container_name: mlflow-serve
    depends_on:
      mlflow:
        condition: service_healthy
    environment:
      - "MLFLOW_TRACKING_URI=http://mlflow:5000"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, python, -c, "import urllib.request; urllib.request.urlopen('http://localhost:5001/health')"]
      timeout: 5s
    image: "ghcr.io/mlflow/mlflow:${MLFLOW_VERSION:-v3.5.0}"
    ports:
      - "${MLFLOW_SERVE_PORT:-5001}:5001"
  mongodb:
    command: [/bin/sh, -c, /opt/app/my_data.sh]
    container_name: mongodb-connect
    depends_on:
      - mongodb-server
    environment:
      - "CONN_STR=mongodb://${MONGODB_USER:-user}:${MONGODB_PASSWORD:-password}@mongodb-server"
    image: "mongodb/mongodb-community-server:${MONGODB_VERSION:-8.2.1-ubi8}"
    volumes:
      - "./data/mongodb:/opt/app"
  mongodb-server:
    container_name: mongodb
    environment:
      - "MONGO_INITDB_ROOT_USERNAME=${MONGODB_USER:-user}"
      - "MONGO_INITDB_ROOT_PASSWORD=${MONGODB_PASSWORD:-password}"
    image: "mongo:${MONGODB_VERSION:-8.0.15}"
    ports:
      - "27017:27017"
  mssql:
    container_name: mssql
    environment:
      - "SA_PASSWORD=${MSSQL_PASSWORD:-yourStrong(!)Password}"
      - ACCEPT_EULA=Y
    healthcheck:
      interval: 10s
      retries: 10
      test: [CMD-SHELL, mssql-health-check]
      timeout: 10s
    image: "mcr.microsoft.com/mssql/server:${MSSQL_VERSION:-2025-latest}"
    ports:
      - "1433:1433"
    volumes:
      - "./data/mssql/mssql-health-check:/usr/local/bin/mssql-health-check"
  mysql:
    command: [/bin/bash, -c, /tmp/scripts/init.sh]
    container_name: mysql-data
    depends_on:
      mysql-server:
        condition: service_healthy
    environment:
      - "MYSQL_PASSWORD=${MYSQL_PASSWORD:-root}"
    image: "mysql:${MYSQL_VERSION:-9.4.0}"
    volumes:
      - "./data/mysql/init.sh:/tmp/scripts/init.sh"
      - "${MYSQL_DATA:-./data/mysql/data}:/tmp/data"
  mysql-server:
    container_name: mysql
    environment:
      - "MYSQL_ROOT_PASSWORD=${MYSQL_PASSWORD:-root}"
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, mysqladmin, ping, -h, localhost, -u, root, -p$$MYSQL_ROOT_PASSWORD]
      timeout: 5s
    image: "mysql:${MYSQL_VERSION:-9.4.0}"
    ports:
      - "3306:3306"
  nats:
    command: [--http_port, "8222"]
    container_name: nats
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, /bin/sh, -c, "curl -f http://localhost:8222/healthz"]
      timeout: 5s
    image: "nats:${NATS_VERSION:-2.12.1}"
    ports:
      - "${NATS_PORT:-4222}:4222"
      - "8222:8222"
  neo4j:
    container_name: neo4j
    environment:
      - NEO4J_AUTH=none
    healthcheck:
      interval: 30s
      retries: 5
      test: [CMD-SHELL, "cypher-shell -u neo4j -p test 'RETURN 1' || exit 1"]
      timeout: 10s
    image: "neo4j:${NEO4J_VERSION:-2025.09.0}"
    ports:
      - "7474:7474"
      - "7687:7687"
  openmetadata:
    command:
      - /opt/airflow/ingestion_dependency.sh
    container_name: openmetadata-ingestion
    depends_on:
      openmetadata-server:
        condition: service_healthy
    entrypoint: /bin/bash
    env_file: ./data/openmetadata-ingestion/env/docker.env
    environment:
      - "DB_USER=${MYSQL_USER:-root}"
      - "DB_USER_PASSWORD=${MYSQL_PASSWORD:-root}"
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 30s
      test: [CMD, curl, --fail, "http://localhost:8080/health"]
      timeout: 10s
    image: "docker.getcollate.io/openmetadata/ingestion:${OPENMETADATA_VERSION:-1.10.1}"
    ports:
      - "8080:8080"
  openmetadata-server:
    container_name: openmetadata
    depends_on:
      openmetadata-setup:
        condition: service_completed_successfully
    env_file: ./data/openmetadata/env/docker.env
    environment:
      - "DB_USER=${MYSQL_USER:-root}"
      - "DB_USER_PASSWORD=${MYSQL_PASSWORD:-root}"
      - "ELASTICSEARCH_USER=${ELASTICSEARCH_USER:-elastic}"
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD, wget, -q, --spider,  "http://localhost:8586/healthcheck"]
      timeout: 5s
    image: "docker.getcollate.io/openmetadata/server:${OPENMETADATA_VERSION:-1.10.1}"
    ports:
      - "8585:8585"
      - "8586:8586"
    restart: always
  openmetadata-setup:
    command: ./bootstrap/openmetadata-ops.sh migrate
    container_name: openmetadata-setup
    depends_on:
      elasticsearch:
        condition: service_completed_successfully
      mysql:
        condition: service_completed_successfully
    env_file: ./data/openmetadata/env/docker.env
    environment:
      - "DB_USER=${MYSQL_USER:-root}"
      - "DB_USER_PASSWORD=${MYSQL_PASSWORD:-root}"
      - "ELASTICSEARCH_USER=${ELASTICSEARCH_USER:-elastic}"
      - "ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-elasticsearch}"
    image: "docker.getcollate.io/openmetadata/server:${OPENMETADATA_VERSION:-1.10.1}"
  opensearch:
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - "OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_PASSWORD:-!BigData#1}"
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD, curl, --fail, "https://localhost:9200", -ku, "admin:${OPENSEARCH_PASSWORD:-!BigData#1}"]
      timeout: 5s
    image: "opensearchproject/opensearch:${OPENSEARCH_VERSION:-3.3.0}"
    ports:
      - "9600:9600"
      - "9200:9200"
  pinot:
    command: "StartServer -zkAddress zookeeper:2181"
    container_name: pinot-server
    depends_on:
      pinot-broker:
        condition: service_healthy
    environment:
      - "JAVA_OPTS=-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx16G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-server.log"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8098/health/readiness"]
      timeout: 5s
    image: "apachepinot/pinot:${PINOT_VERSION:-1.4.0}"
    ports:
      - "8098:8098"
    restart: unless-stopped
  pinot-broker:
    command: "StartBroker -zkAddress zookeeper:2181"
    container_name: pinot-broker
    depends_on:
      pinot-controller:
        condition: service_healthy
    environment:
      - "JAVA_OPTS=-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-broker.log"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8099/health"]
      timeout: 5s
    image: "apachepinot/pinot:${PINOT_VERSION:-1.4.0}"
    ports:
      - "8099:8099"
    restart: unless-stopped
  pinot-controller:
    command: "StartController -zkAddress zookeeper:2181"
    container_name: pinot
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      - "JAVA_OPTS=-Dplugins.dir=/opt/pinot/plugins -Xms1G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-controller.log"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:9000/pinot-controller/admin"]
      timeout: 5s
    image: "apachepinot/pinot:${PINOT_VERSION:-1.4.0}"
    ports:
      - "9000:9000"
    restart: unless-stopped
  polaris:
    container_name: polaris
    healthcheck:
      interval: 10s
      retries: 5
      test: [CMD, curl, "http://localhost:8182/healthcheck"]
      timeout: 10s
    image: "datacatering/polaris:${POLARIS_VERSION:-1.0.0}"
    ports:
      - "8181:8181"
      - "8182:8182"
  postgres:
    command: [/bin/bash, -c, /tmp/scripts/init.sh]
    container_name: postgres-data
    depends_on:
      postgres-server:
        condition: service_healthy
    environment:
      - "POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "PGPASSWORD=${POSTGRES_PASSWORD:-postgres}"
    image: "postgres:${POSTGRES_VERSION:-18.0}"
    volumes:
      - "./data/postgres/init.sh:/tmp/scripts/init.sh"
      - "${POSTGRES_DATA:-./data/postgres/data}:/tmp/data"
  postgres-server:
    container_name: postgres
    environment:
      - "POSTGRES_USER=${POSTGRES_USER:-postgres}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}"
      - PGDATA=/data/postgres
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, pg_isready]
      timeout: 5s
    image: "postgres:${POSTGRES_VERSION:-18.0}"
    ports:
      - "5432:5432"
  prefect:
    container_name: prefect-data
    depends_on:
      - prefect-server
    entrypoint: [/opt/prefect/app/start_flows.sh]
    environment:
      - "PREFECT_API_URL=http://host.docker.internal:4200/api"
    image: "prefecthq/prefect:${PREFECT_VERSION:-3.4.24-python3.11}"
    volumes:
      - "./data/prefect/flows:/root/flows"
      - "./data/prefect/start_flows.sh:/opt/prefect/app/start_flows.sh"
    working_dir: /root/flows
  prefect-server:
    container_name: prefect
    depends_on:
      postgres:
        condition: service_completed_successfully
    entrypoint: [/opt/prefect/entrypoint.sh, prefect, server, start]
    environment:
      - "PREFECT_UI_URL=http://127.0.0.1:4200/api"
      - "PREFECT_API_URL=http://127.0.0.1:4200/api"
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - "PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/prefect"
    image: "prefecthq/prefect:${PREFECT_VERSION:-3.4.24-python3.11}"
    ports:
      - "4200:4200"
    restart: always
  presto:
    command: [/opt/presto-server/bin/launcher, run]
    container_name: presto
    image: "ahanaio/prestodb:${PRESTO_VERSION:-0.284}"
    ports:
      - "8083:8080"
    volumes:
      - "./data/presto/etc:/opt/presto-server/etc"
      - "./data/presto/catalog:/opt/presto-server/etc/catalog"
  prometheus:
    command: --config.file=/etc/prometheus/prometheus.yml
    container_name: prometheus
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, wget, --no-verbose, --tries=1, --spider, "http://localhost:9090/-/healthy"]
      timeout: 5s
    image: "prom/prometheus:${PROMETHEUS_VERSION:-v3.7.0}"
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - "./data/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml"
  pulsar:
    command: [bin/pulsar, standalone]
    container_name: pulsar
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD, bin/pulsar-admin, brokers, healthcheck]
      timeout: 10s
    image: "apachepulsar/pulsar:${PULSAR_VERSION:-4.1.1}"
    ports:
      - "${PULSAR_PORT:-6650}:6650"
      - "8080:8080"
    volumes:
      - "./data/pulsar/conf:/pulsar/conf"
  qdrant:
    container_name: qdrant
    environment:
      - QDRANT_ALLOW_RECOVERY_MODE=true
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, -f, "http://localhost:6333/health"]
      timeout: 5s
    image: "qdrant/qdrant:${QDRANT_VERSION:-v1.15.5}"
    ports:
      - "${QDRANT_PORT:-6333}:6333"
      - "6334:6334"
  rabbitmq:
    container_name: rabbitmq
    environment:
      - "RABBITMQ_DEFAULT_USER=${RABBITMQ_USER:-guest}"
      - "RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD:-guest}"
    healthcheck:
      interval: 30s
      retries: 3
      test: rabbitmq-diagnostics -q ping
      timeout: 30s
    hostname: my-rabbit
    image: "rabbitmq:${RABBITMQ_VERSION:-4.1.4-management}"
    ports:
      - "5672:5672"
      - "15672:15672"
  ray:
    command: [bash, -c, ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265 --metrics-export-port=8080 --block]
    container_name: ray
    depends_on:
      grafana:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    env_file: ./data/ray/.env
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, ray, status]
      timeout: 10s
    image: "rayproject/ray:${RAY_VERSION:-2.50.0-aarch64}"
    ports:
      - "${RAY_PORT:-8265}:8265"
      - "6379:6379"
      - "8080:8080"
    shm_size: 4gb
    ulimits:
      memlock: -1
      nofile:
        hard: 65536
        soft: 65536
  redash:
    command: scheduler
    container_name: redash-scheduler
    depends_on:
      postgres:
        condition: service_completed_successfully
      redash-server:
        condition: service_healthy
      redis:
        condition: service_healthy
    env_file: ./data/redash/.env
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD, /bin/sh, -c, /app/bin/docker-entrypoint workers_healthcheck]
      timeout: 5s
    image: "redash/redash:${REDASH_VERSION:-10.1.0.b50633}"
  redash-data:
    command: [/app/bin/docker-entrypoint, create_db]
    container_name: redash-data
    env_file: ./data/redash/.env
    image: "redash/redash:${REDASH_VERSION:-10.1.0.b50633}"
  redash-server:
    command: server
    container_name: redash
    depends_on:
      redash-data:
        condition: service_completed_successfully
    env_file: ./data/redash/.env
    healthcheck:
      interval: 5s
      retries: 5
      test: [CMD, curl, --output, /dev/null, --silent, --head, --fail, http://localhost:5000/ping]
      timeout: 5s
    image: "redash/redash:${REDASH_VERSION:-10.1.0.b50633}"
    ports:
      - "5000:5000"
  redis:
    container_name: redis
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, redis-cli, --raw, incr, ping]
      timeout: 5s
    image: "redis:${REDIS_VERSION:-8.2.2}"
    ports:
      - "6379:6379"
  solace:
    container_name: solace-data
    depends_on:
      solace-server:
        condition: service_healthy
    entrypoint: [/bin/sh, -c, /opt/app/my_data.sh]
    image: "solace/solace-pubsub-standard:${SOLACE_VERSION:-10.12}"
    volumes:
      - "./data/solace:/opt/app"
  solace-server:
    container_name: solace
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 1
    environment:
      - username_admin_globalaccesslevel=admin
      - "username_admin_password=${SOLACE_PASSWORD:-admin}"
      - system_scaling_maxconnectioncount=100
    healthcheck:
      interval: 30s
      retries: 3
      test: [CMD, curl, --output, /dev/null, --silent, --head, --fail, "http://localhost:8080"]
      timeout: 5s
    image: "solace/solace-pubsub-standard:${SOLACE_VERSION:-10.12}"
    ports:
      - "8080:8080"
      - "55554:55555"
    shm_size: 1g
    ulimits:
      core: -1
      nofile:
        hard: 6592
        soft: 2448
  sonarqube:
    container_name: sonarqube
    environment:
      - SONAR_WEB_CONTEXT=/sonar
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, "wget --spider localhost:9000/sonar"]      
      timeout: 10s
    image: "sonarqube:${SONARQUBE_VERSION:-lts-community}"
    ports:
      - "9000:9000"
  spanner:
    container_name: spanner
    image: "gcr.io/cloud-spanner-emulator/emulator:${SPANNER_VERSION:-1.5.42}"
    ports:
      - "9010:9010"
      - "9020:9020"
  sqlite:
    command: [tail, -f, /dev/null]
    container_name: sqlite
    image: "keinos/sqlite3:${SQLITE_VERSION:-3.50.4}"
    volumes:
      - "./data/sqlite:/opt/data"
  superset:
    command: [/app/docker/docker-init.sh]
    container_name: superset_init
    depends_on:
      postgres:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      superset-server:
        condition: service_healthy
    env_file: ./data/superset/docker/.env
    healthcheck:
      disable: true
    image: "apache/superset:${SUPERSET_VERSION:-v5.0.0}"
    user: root
    volumes:
      - "./data/superset/docker:/app/docker"
  superset-server:
    command: [/app/docker/docker-bootstrap.sh, app-gunicorn]
    container_name: superset
    env_file: ./data/superset/docker/.env
    healthcheck:
      interval: 5s
      retries: 3
      test: [CMD, curl, --output, /dev/null, --silent, --head, --fail, "http://localhost:8088/health"]
      timeout: 5s
    image: "apache/superset:${SUPERSET_VERSION:-v5.0.0}"
    ports:
      - "8088:8088"
    user: root
    volumes:
      - "./data/superset/docker:/app/docker"
  temporal:
    command: [server, start-dev, --db-filename, /opt/data/db/temporal.db, --ip, 0.0.0.0, --metrics-port, "9233"]
    container_name: temporal
    entrypoint: temporal
    environment: []
    expose:
      - 8233
      - 7233
    image: "temporalio/server:${TEMPORAL_VERSION:-1.28.1.0}"
    ports:
      - "8233:8233"
      - "7233:7233"
      - "9233:9233"
  timescaledb:
    container_name: timescaledb
    environment:
      - "POSTGRES_USER=${TIMESCALEDB_USER:-postgres}"
      - "POSTGRES_PASSWORD=${TIMESCALEDB_PASSWORD:-postgres}"
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD-SHELL, pg_isready -U postgres]
      timeout: 5s
    image: "timescale/timescaledb:${TIMESCALEDB_VERSION:-latest-pg15}"
    ports:
      - "${TIMESCALEDB_PORT:-5432}:5432"
  trino:
    container_name: trino
    depends_on:
      postgres:
        condition: service_completed_successfully
    image: "trinodb/trino:${TRINO_VERSION:-477}"
    ports:
      - "8084:8080"
    volumes:
      - "./data/trino/etc:/usr/lib/trino/etc:ro"
      - "./data/trino/catalog:/etc/trino/catalog"
  unitycatalog:
    container_name: unitycatalog
    image: "datacatering/unitycatalog:${UNITYCATALOG_VERSION:-0.1.0}"
    ports:
      - "8081:8081"
  vault:
    cap_add:
      - IPC_LOCK
    container_name: vault
    environment:
      - VAULT_DEV_ROOT_TOKEN_ID=root
      - VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, vault, status]
      timeout: 5s
    image: "hashicorp/vault:${VAULT_VERSION:-1.20}"
    ports:
      - "${VAULT_PORT:-8200}:8200"
  weaviate:
    container_name: weaviate
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      CLUSTER_HOSTNAME: node1
      DEFAULT_VECTORIZER_MODULE: none
      QUERY_DEFAULTS_LIMIT: 25
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, -f, "http://localhost:8080/v1/.well-known/ready"]
      timeout: 5s
    image: "semitechnologies/weaviate:${WEAVIATE_VERSION:-1.33.0}"
    ports:
      - "${WEAVIATE_PORT:-8080}:8080"
  zookeeper:
    container_name: zookeeper
    environment:
      - ZOO_MY_ID=1
    healthcheck:
      interval: 5s
      retries: 3
      test: "nc -z localhost 2181 || exit -1"
      timeout: 5s
    image: "zookeeper:${ZOOKEEPER_VERSION:-3.9.4}"
    ports:
      - "2181:2181"
